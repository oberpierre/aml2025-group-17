{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9fab8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_11040\\3870842209.py\", line 7, in <module>\n",
      "    from datasets import load_dataset\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py\", line 17, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 56, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 52, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_11040\\3870842209.py\", line 7, in <module>\n",
      "    from datasets import load_dataset\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py\", line 17, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 56, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 66, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py\", line 61, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\micha\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afb498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/ontonotes_embeddings_full.npz')\n",
    "ontonotes = load_dataset(\"conll2012_ontonotesv5\", \"english_v12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30fc9704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys: KeysView(NpzFile 'data/ontonotes_embeddings_full.npz' with keys: X, Y)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data keys: {data.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1ceb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2200865, 768)\n",
      "y shape: (2200865,)\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89691fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 125904, Negatives: 2074961\n"
     ]
    }
   ],
   "source": [
    "num_positives = np.sum(y == 1)\n",
    "num_negatives = np.sum(y == 0)\n",
    "print(f\"Positives: {num_positives}, Negatives: {num_negatives}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0287e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part fries my computer, so I limit the dataset to 200k samples, but still does\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Create Dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split into training and validation sets, later on we use the real validation set, but for now...\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3352da12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200865\n",
      "2200865\n"
     ]
    }
   ],
   "source": [
    "print(len(X_tensor))\n",
    "print(len(y_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a1d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class confidence_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(confidence_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d62c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.7105\n",
      "Epoch 2, Training Loss: 0.6100\n",
      "Epoch 3, Training Loss: 0.5768\n",
      "Epoch 4, Training Loss: 0.5594\n",
      "Epoch 5, Training Loss: 0.5460\n",
      "Epoch 6, Training Loss: 0.5366\n",
      "Epoch 7, Training Loss: 0.5294\n",
      "Epoch 8, Training Loss: 0.5246\n",
      "Epoch 9, Training Loss: 0.5173\n",
      "Epoch 10, Training Loss: 0.5125\n",
      "Epoch 11, Training Loss: 0.5095\n",
      "Epoch 12, Training Loss: 0.5063\n",
      "Epoch 13, Training Loss: 0.5045\n",
      "Epoch 14, Training Loss: 0.4976\n",
      "Epoch 15, Training Loss: 0.4958\n",
      "Epoch 16, Training Loss: 0.4958\n",
      "Epoch 17, Training Loss: 0.4873\n",
      "Epoch 18, Training Loss: 0.4855\n",
      "Epoch 19, Training Loss: 0.4841\n",
      "Epoch 20, Training Loss: 0.4817\n",
      "Epoch 21, Training Loss: 0.4811\n",
      "Epoch 22, Training Loss: 0.4806\n",
      "Epoch 23, Training Loss: 0.4772\n",
      "Epoch 24, Training Loss: 0.4785\n",
      "Epoch 25, Training Loss: 0.4739\n",
      "Epoch 26, Training Loss: 0.4727\n",
      "Epoch 27, Training Loss: 0.4676\n",
      "Epoch 28, Training Loss: 0.4710\n",
      "Epoch 29, Training Loss: 0.4656\n",
      "Epoch 30, Training Loss: 0.4658\n"
     ]
    }
   ],
   "source": [
    "model = confidence_model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# because this dataset is unbalanced, we use a weighted loss function\n",
    "pos_weight = torch.tensor([num_negatives / num_positives], dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "torch.save(model.state_dict(), \"confidence_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670ea0d",
   "metadata": {},
   "source": [
    "# Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fe2f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a58bc0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: Dataset({\n",
      "    features: ['document_id', 'sentences'],\n",
      "    num_rows: 1200\n",
      "})\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.']\n",
      "{'part_id': 0, 'words': ['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.'], 'pos_tags': [9, 33, 5, 31, 41, 33, 43, 18, 18, 14, 19, 19, 28, 8], 'parse_tree': '(TOP(S (: --) (ADVP (RB basically) ) (, ,) (NP (PRP it) )(VP (VBD was) (ADVP (RB unanimously) )(VP (VBN agreed) (PP (IN upon) )(PP (IN by) (NP (DT the)  (JJ various)  (JJ relevant)  (NNS parties) )))) (. .) ))', 'predicate_lemmas': [None, None, None, None, 'be', None, 'agree', None, None, None, None, None, None, None], 'predicate_framenet_ids': [None, None, None, None, '03', None, '01', None, None, None, None, None, None, None], 'word_senses': [None, None, None, None, None, None, 1.0, None, None, None, None, None, None, None], 'speaker': 'speaker#1', 'named_entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'srl_frames': [{'verb': 'was', 'frames': ['O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'agreed', 'frames': ['O', 'B-ARGM-ADV', 'O', 'O', 'O', 'B-ARGM-MNR', 'B-V', 'B-ARG1', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'O']}], 'coref_spans': []}\n"
     ]
    }
   ],
   "source": [
    "test_data = ontonotes[\"test\"]\n",
    "print(f\"test_data: {test_data}\")\n",
    "print((test_data[\"sentences\"][0][0][\"words\"]))\n",
    "print(test_data[\"sentences\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1c018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "confidence_model(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name)\n",
    "embedding_model.eval()\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_model.eval()\n",
    "\n",
    "# model = confidence_model()\n",
    "# model.load_state_dict(torch.load(\"confidence_model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43511798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence_model(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_model.to(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba37ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions_to_words(predicted_token_ids, word_ids):\n",
    "    \"\"\"\n",
    "    Align token-level predictions back to word-level by choosing\n",
    "    the first subtoken's prediction for each word.\n",
    "    \"\"\"\n",
    "    aligned_preds = []\n",
    "    prev_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        if word_idx != prev_word_idx:\n",
    "            aligned_preds.append(predicted_token_ids[idx])\n",
    "            prev_word_idx = word_idx\n",
    "    return aligned_preds\n",
    "\n",
    "def process_sentence_with_confidence_model(words):\n",
    "    x_list, y_list = [], []\n",
    "    ner_tags_all = []\n",
    "\n",
    "    for i in range(1, len(words) + 1):\n",
    "        partial = words[:i]\n",
    "\n",
    "        # Tokenize and get embeddings\n",
    "        inputs = tokenizer(\" \".join(partial), return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = embedding_model(**inputs)\n",
    "            cls = outputs.last_hidden_state[0][0].unsqueeze(0)  # [1, 768]\n",
    "\n",
    "        # Get NER tags for the partial using BERT-NER model\n",
    "        with torch.no_grad():\n",
    "            ner_outputs = ner_model(**inputs)\n",
    "            logits = ner_outputs.logits\n",
    "            predicted_token_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "        word_ids = inputs.word_ids()\n",
    "        word_level_pred_ids = align_predictions_to_words(predicted_token_ids, word_ids)\n",
    "        label_list = [ner_model.config.id2label[i] for i in range(len(ner_model.config.id2label))]\n",
    "        ner_tags = [label_list[i] for i in word_level_pred_ids]\n",
    "\n",
    "        ner_tags_all.extend(ner_tags)\n",
    "\n",
    "        # Confidence model prediction\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(cls)\n",
    "            y_pred = torch.sigmoid(y_pred).item()\n",
    "\n",
    "        x_list.append(cls.cpu().numpy())\n",
    "        y_list.append(1 if y_pred > 0.8 else 0)\n",
    "\n",
    "    # Unique tags in sentence (no duplicates)\n",
    "    unique_tags = list(dict.fromkeys(ner_tags_all))\n",
    "\n",
    "    # Only keep partials where entity ends\n",
    "    entity_partials = [words[:i] for i in range(1, len(words)+1) if y_list[i-1]==1]\n",
    "\n",
    "    return (unique_tags, entity_partials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a170add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Barack Obama was born in Hawaii\n",
      "Result tuple:\n",
      "Unique tags: ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
      "Entity-ending partials:\n",
      "['Barack']\n",
      "['Barack', 'Obama']\n",
      "['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii']\n"
     ]
    }
   ],
   "source": [
    "dummy_sentence = [\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\"]\n",
    "\n",
    "result = process_sentence_with_confidence_model(dummy_sentence)\n",
    "\n",
    "print(\"Input sentence:\", \" \".join(dummy_sentence))\n",
    "print(\"Result tuple:\")\n",
    "print(\"Unique tags:\", result[0])\n",
    "print(\"Entity-ending partials:\")\n",
    "for partial in result[1]:\n",
    "    print(partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5ae70e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents:   0%|          | 0/1200 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence_dict \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      6\u001b[0m         words \u001b[38;5;241m=\u001b[39m sentence_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m         result \u001b[38;5;241m=\u001b[39m process_sentence_with_confidence_model(words)\n\u001b[0;32m      8\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Save as npz\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m, in \u001b[0;36mprocess_sentence_with_confidence_model\u001b[1;34m(words)\u001b[0m\n\u001b[0;32m     21\u001b[0m partial \u001b[38;5;241m=\u001b[39m words[:i]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Tokenize and get embeddings\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(partial), return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     26\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m embedding_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:810\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[1;34m(self, device, non_blocking)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    811\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    813\u001b[0m     }\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:811\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 811\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    813\u001b[0m     }\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over test data\n",
    "results = []\n",
    "\n",
    "for row in tqdm(test_data, desc=\"Documents\"):\n",
    "    for sentence_dict in row[\"sentences\"]:\n",
    "        words = sentence_dict[\"words\"]\n",
    "        result = process_sentence_with_confidence_model(words)\n",
    "        results.append(result)\n",
    "\n",
    "# Save as npz\n",
    "results_array = np.array(results, dtype=object)\n",
    "np.savez_compressed(\"confidence_model_evaluation_results.npz\", data=results_array)\n",
    "\n",
    "print(f\"Processed {len(results)} sentences with confidence model and saved results to confidence_model_evaluation_results.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b4cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
