{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f2857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f3395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/ner_trigger_dataset_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0302c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys: KeysView(NpzFile 'data/ner_trigger_dataset_embeddings.npz' with keys: X, y)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data keys: {data.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769b0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2148223, 768)\n",
      "y shape: (2148223,)\n"
     ]
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec28660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 519979, Negatives: 1628244\n"
     ]
    }
   ],
   "source": [
    "num_positives = np.sum(y == 1)\n",
    "num_negatives = np.sum(y == 0)\n",
    "print(f\"Positives: {num_positives}, Negatives: {num_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64f21df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw X[50]: ['of' 'a' 'primary' 'stele' ',' 'secondary'], Raw y[50]: 0\n",
      "Raw X[51]: ['a' 'primary' 'stele' ',' 'secondary' 'steles'], Raw y[51]: 0\n",
      "Raw X[52]: ['primary' 'stele' ',' 'secondary' 'steles' ','], Raw y[52]: 0\n",
      "Raw X[53]: ['stele' ',' 'secondary' 'steles' ',' 'a'], Raw y[53]: 0\n",
      "Raw X[54]: [',' 'secondary' 'steles' ',' 'a' 'huge'], Raw y[54]: 0\n",
      "Raw X[55]: ['secondary' 'steles' ',' 'a' 'huge' 'round'], Raw y[55]: 0\n",
      "Raw X[56]: ['steles' ',' 'a' 'huge' 'round' 'sculpture'], Raw y[56]: 0\n",
      "Raw X[57]: [',' 'a' 'huge' 'round' 'sculpture' 'and'], Raw y[57]: 0\n",
      "Raw X[58]: ['a' 'huge' 'round' 'sculpture' 'and' 'beacon'], Raw y[58]: 0\n",
      "Raw X[59]: ['huge' 'round' 'sculpture' 'and' 'beacon' 'tower'], Raw y[59]: 0\n",
      "Raw X[60]: ['round' 'sculpture' 'and' 'beacon' 'tower' ','], Raw y[60]: 0\n",
      "Raw X[61]: ['sculpture' 'and' 'beacon' 'tower' ',' 'and'], Raw y[61]: 0\n",
      "Raw X[62]: ['and' 'beacon' 'tower' ',' 'and' 'the'], Raw y[62]: 0\n",
      "Raw X[63]: ['beacon' 'tower' ',' 'and' 'the' 'Great'], Raw y[63]: 0\n",
      "Raw X[64]: ['tower' ',' 'and' 'the' 'Great' 'Wall'], Raw y[64]: 1\n",
      "Raw X[65]: [',' 'and' 'the' 'Great' 'Wall' ','], Raw y[65]: 1\n",
      "Raw X[66]: ['and' 'the' 'Great' 'Wall' ',' 'among'], Raw y[66]: 1\n",
      "Raw X[67]: ['the' 'Great' 'Wall' ',' 'among' 'other'], Raw y[67]: 1\n",
      "Raw X[68]: ['Great' 'Wall' ',' 'among' 'other' 'things'], Raw y[68]: 0\n",
      "Raw X[69]: ['Wall' ',' 'among' 'other' 'things' '.'], Raw y[69]: 0\n",
      "Raw X[70]: [',' 'among' 'other' 'things' '.' 'A'], Raw y[70]: 0\n",
      "Raw X[71]: ['among' 'other' 'things' '.' 'A' 'primary'], Raw y[71]: 0\n",
      "Raw X[72]: ['other' 'things' '.' 'A' 'primary' 'stele'], Raw y[72]: 0\n",
      "Raw X[73]: ['things' '.' 'A' 'primary' 'stele' ','], Raw y[73]: 0\n",
      "Raw X[74]: ['.' 'A' 'primary' 'stele' ',' 'three'], Raw y[74]: 1\n",
      "Raw X[75]: ['A' 'primary' 'stele' ',' 'three' 'secondary'], Raw y[75]: 1\n",
      "Raw X[76]: ['primary' 'stele' ',' 'three' 'secondary' 'steles'], Raw y[76]: 1\n",
      "Raw X[77]: ['stele' ',' 'three' 'secondary' 'steles' ','], Raw y[77]: 1\n",
      "Raw X[78]: [',' 'three' 'secondary' 'steles' ',' 'and'], Raw y[78]: 1\n",
      "Raw X[79]: ['three' 'secondary' 'steles' ',' 'and' 'two'], Raw y[79]: 1\n",
      "Raw X[80]: ['secondary' 'steles' ',' 'and' 'two' 'inscribed'], Raw y[80]: 1\n",
      "Raw X[81]: ['steles' ',' 'and' 'two' 'inscribed' 'steles'], Raw y[81]: 1\n",
      "Raw X[82]: [',' 'and' 'two' 'inscribed' 'steles' '.'], Raw y[82]: 1\n",
      "Raw X[83]: ['and' 'two' 'inscribed' 'steles' '.' 'The'], Raw y[83]: 0\n",
      "Raw X[84]: ['two' 'inscribed' 'steles' '.' 'The' 'Hundred'], Raw y[84]: 0\n",
      "Raw X[85]: ['inscribed' 'steles' '.' 'The' 'Hundred' 'Regiments'], Raw y[85]: 0\n",
      "Raw X[86]: ['steles' '.' 'The' 'Hundred' 'Regiments' 'Offensive'], Raw y[86]: 1\n",
      "Raw X[87]: ['.' 'The' 'Hundred' 'Regiments' 'Offensive' 'was'], Raw y[87]: 1\n",
      "Raw X[88]: ['The' 'Hundred' 'Regiments' 'Offensive' 'was' 'the'], Raw y[88]: 1\n",
      "Raw X[89]: ['Hundred' 'Regiments' 'Offensive' 'was' 'the' 'campaign'], Raw y[89]: 0\n"
     ]
    }
   ],
   "source": [
    "data_raw = np.load(\"data/ner_trigger_dataset.npz\")\n",
    "X_raw = data_raw['X']\n",
    "y_raw = data_raw['y']\n",
    "for i in range(40):\n",
    "    print(f\"Raw X[{i+50}]: {X_raw[i+50]}, Raw y[{i+50}]: {y_raw[i+50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af84d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part fries my computer, so I limit the dataset to 200k samples, but still does\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create Dataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split into training and validation sets, later on we use the real validation set, but for now...\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d59e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2148223\n",
      "2148223\n"
     ]
    }
   ],
   "source": [
    "print(len(X_tensor))\n",
    "print(len(y_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8df1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class window_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(window_model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c59c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.5471\n",
      "Epoch 2, Training Loss: 0.4882\n",
      "Epoch 3, Training Loss: 0.4750\n",
      "Epoch 4, Training Loss: 0.4664\n",
      "Epoch 5, Training Loss: 0.4617\n",
      "Epoch 6, Training Loss: 0.4596\n",
      "Epoch 7, Training Loss: 0.4551\n",
      "Epoch 8, Training Loss: 0.4517\n",
      "Epoch 9, Training Loss: 0.4507\n",
      "Epoch 10, Training Loss: 0.4486\n",
      "Epoch 11, Training Loss: 0.4457\n",
      "Epoch 12, Training Loss: 0.4433\n",
      "Epoch 13, Training Loss: 0.4449\n",
      "Epoch 14, Training Loss: 0.4434\n",
      "Epoch 15, Training Loss: 0.4413\n",
      "Epoch 16, Training Loss: 0.4394\n",
      "Epoch 17, Training Loss: 0.4381\n",
      "Epoch 18, Training Loss: 0.4367\n",
      "Epoch 19, Training Loss: 0.4364\n",
      "Epoch 20, Training Loss: 0.4344\n",
      "Epoch 21, Training Loss: 0.4332\n",
      "Epoch 22, Training Loss: 0.4318\n",
      "Epoch 23, Training Loss: 0.4317\n",
      "Epoch 24, Training Loss: 0.4305\n",
      "Epoch 25, Training Loss: 0.4303\n",
      "Epoch 26, Training Loss: 0.4293\n",
      "Epoch 27, Training Loss: 0.4281\n",
      "Epoch 28, Training Loss: 0.4287\n",
      "Epoch 29, Training Loss: 0.4295\n",
      "Epoch 30, Training Loss: 0.4284\n",
      "Epoch 31, Training Loss: 0.4276\n",
      "Epoch 32, Training Loss: 0.4266\n",
      "Epoch 33, Training Loss: 0.4266\n",
      "Epoch 34, Training Loss: 0.4261\n",
      "Epoch 35, Training Loss: 0.4256\n",
      "Epoch 36, Training Loss: 0.4251\n",
      "Epoch 37, Training Loss: 0.4249\n",
      "Epoch 38, Training Loss: 0.4237\n",
      "Epoch 39, Training Loss: 0.4230\n",
      "Epoch 40, Training Loss: 0.4229\n",
      "Epoch 41, Training Loss: 0.4222\n",
      "Epoch 42, Training Loss: 0.4227\n",
      "Epoch 43, Training Loss: 0.4218\n",
      "Epoch 44, Training Loss: 0.4216\n",
      "Epoch 45, Training Loss: 0.4208\n",
      "Epoch 46, Training Loss: 0.4203\n",
      "Epoch 47, Training Loss: 0.4204\n",
      "Epoch 48, Training Loss: 0.4200\n",
      "Epoch 49, Training Loss: 0.4197\n",
      "Epoch 50, Training Loss: 0.4192\n"
     ]
    }
   ],
   "source": [
    "model = window_model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# because this dataset is unbalanced, we use a weighted loss function, maybe not here but it is still implemented.\n",
    "# the ratio is only one to three.\n",
    "pos_weight = torch.tensor([num_negatives / num_positives], dtype=torch.float32).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "Epoch 1, Training Loss: 0.5471\n",
    "Epoch 2, Training Loss: 0.4882\n",
    "Epoch 3, Training Loss: 0.4750\n",
    "Epoch 4, Training Loss: 0.4664\n",
    "Epoch 5, Training Loss: 0.4617\n",
    "Epoch 6, Training Loss: 0.4596\n",
    "Epoch 7, Training Loss: 0.4551\n",
    "Epoch 8, Training Loss: 0.4517\n",
    "Epoch 9, Training Loss: 0.4507\n",
    "Epoch 10, Training Loss: 0.4486\n",
    "Epoch 11, Training Loss: 0.4457\n",
    "Epoch 12, Training Loss: 0.4433\n",
    "Epoch 13, Training Loss: 0.4449\n",
    "Epoch 14, Training Loss: 0.4434\n",
    "Epoch 15, Training Loss: 0.4413\n",
    "Epoch 16, Training Loss: 0.4394\n",
    "Epoch 17, Training Loss: 0.4381\n",
    "Epoch 18, Training Loss: 0.4367\n",
    "Epoch 19, Training Loss: 0.4364\n",
    "Epoch 20, Training Loss: 0.4344\n",
    "Epoch 21, Training Loss: 0.4332\n",
    "Epoch 22, Training Loss: 0.4318\n",
    "Epoch 23, Training Loss: 0.4317\n",
    "Epoch 24, Training Loss: 0.4305\n",
    "Epoch 25, Training Loss: 0.4303\n",
    "Epoch 26, Training Loss: 0.4293\n",
    "Epoch 27, Training Loss: 0.4281\n",
    "Epoch 28, Training Loss: 0.4287\n",
    "Epoch 29, Training Loss: 0.4295\n",
    "Epoch 30, Training Loss: 0.4284\n",
    "Epoch 31, Training Loss: 0.4276\n",
    "Epoch 32, Training Loss: 0.4266\n",
    "Epoch 33, Training Loss: 0.4266\n",
    "Epoch 34, Training Loss: 0.4261\n",
    "Epoch 35, Training Loss: 0.4256\n",
    "Epoch 36, Training Loss: 0.4251\n",
    "Epoch 37, Training Loss: 0.4249\n",
    "Epoch 38, Training Loss: 0.4237\n",
    "Epoch 39, Training Loss: 0.4230\n",
    "Epoch 40, Training Loss: 0.4229\n",
    "Epoch 41, Training Loss: 0.4222\n",
    "Epoch 42, Training Loss: 0.4227\n",
    "Epoch 43, Training Loss: 0.4218\n",
    "Epoch 44, Training Loss: 0.4216\n",
    "Epoch 45, Training Loss: 0.4208\n",
    "Epoch 46, Training Loss: 0.4203\n",
    "Epoch 47, Training Loss: 0.4204\n",
    "Epoch 48, Training Loss: 0.4200\n",
    "Epoch 49, Training Loss: 0.4197\n",
    "Epoch 50, Training Loss: 0.4192\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef28b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8937\n",
      "Precision: 0.7897, Recall: 0.7639, F1-score: 0.7766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(X_batch)\n",
    "        probs = torch.sigmoid(logits)  \n",
    "        predicted = (probs > 0.8).float()\n",
    "\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85239026",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa32725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac821578",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model = AutoModel.from_pretrained(\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f70818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(\"data/ner_trigger_dataset_validation.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c81b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sentence, model, window_size=6, tokenizer=tokenizer, bert_model=bert_model, threshold=0.8):\n",
    "    model.eval()\n",
    "    words = sentence.strip().split()\n",
    "    \n",
    "    for i in range(len(words) - window_size + 1):\n",
    "        window_words = words[i:i+window_size]\n",
    "        window = \" \".join(window_words)\n",
    "\n",
    "\n",
    "        inputs = tokenizer(window, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            cls_token = outputs.last_hidden_state[:, 0, :]  \n",
    "            \n",
    "            logits = model(cls_token)\n",
    "            prob = torch.sigmoid(logits).item()       \n",
    "            label = int(prob > threshold)\n",
    "            print(f\"Window {i}: {window}\")\n",
    "            print(f\"Probability: {prob:.4f}, Label: {label}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24fbd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window 0: On August 17 , the Taiwan\n",
      "Probability: 0.8930, Label: 1\n",
      "Window 1: August 17 , the Taiwan military\n",
      "Probability: 0.9952, Label: 1\n",
      "Window 2: 17 , the Taiwan military held\n",
      "Probability: 0.9769, Label: 1\n",
      "Window 3: , the Taiwan military held the\n",
      "Probability: 0.9644, Label: 1\n",
      "Window 4: the Taiwan military held the Lianhsing\n",
      "Probability: 0.8235, Label: 1\n",
      "Window 5: Taiwan military held the Lianhsing 94\n",
      "Probability: 0.5979, Label: 0\n",
      "Window 6: military held the Lianhsing 94 amphibious\n",
      "Probability: 0.8810, Label: 1\n",
      "Window 7: held the Lianhsing 94 amphibious landing\n",
      "Probability: 0.9220, Label: 1\n",
      "Window 8: the Lianhsing 94 amphibious landing exercise\n",
      "Probability: 0.9233, Label: 1\n",
      "Window 9: Lianhsing 94 amphibious landing exercise ,\n",
      "Probability: 0.9280, Label: 1\n",
      "Window 10: 94 amphibious landing exercise , testing\n",
      "Probability: 0.8432, Label: 1\n",
      "Window 11: amphibious landing exercise , testing and\n",
      "Probability: 0.2078, Label: 0\n",
      "Window 12: landing exercise , testing and enhancing\n",
      "Probability: 0.0006, Label: 0\n",
      "Window 13: exercise , testing and enhancing the\n",
      "Probability: 0.0008, Label: 0\n",
      "Window 14: , testing and enhancing the army\n",
      "Probability: 0.1312, Label: 0\n",
      "Window 15: testing and enhancing the army 's\n",
      "Probability: 0.0073, Label: 0\n",
      "Window 16: and enhancing the army 's response\n",
      "Probability: 0.0477, Label: 0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"On August 17 , the Taiwan military held the Lianhsing 94 amphibious landing exercise , testing and enhancing the army 's response\"\n",
    "inference(sentence, model)  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8adbeac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Inference Evaluation Summary ===\n",
      "Total Timesteps: 2148223\n",
      "BERT Calls (Baseline): 2148223\n",
      "BERT Calls (Classifier-Gated): 504149\n",
      "Reduction in BERT Calls: 76.53%\n",
      "Entity Completion Recall (Y=1 captured): 76.95%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_tensor)\n",
    "    probs = torch.sigmoid(logits).squeeze()\n",
    "\n",
    "threshold = 0.8\n",
    "Y_pred = (probs >= threshold).int().numpy()\n",
    "total_timesteps = len(y)\n",
    "bert_calls_baseline = total_timesteps\n",
    "bert_calls_classifier = Y_pred.sum()\n",
    "reduction = 1 - (bert_calls_classifier / bert_calls_baseline)\n",
    "\n",
    "true_positives_captured = sum(\n",
    "    1 for yt, yp in zip(y, Y_pred) if yt == 1 and yp == 1\n",
    ")\n",
    "total_entity_completions = sum(yt == 1 for yt in y)\n",
    "\n",
    "entity_recall = true_positives_captured / total_entity_completions if total_entity_completions > 0 else 0\n",
    "\n",
    "\n",
    "print(\"=== Inference Evaluation Summary ===\")\n",
    "print(f\"Total Timesteps: {total_timesteps}\")\n",
    "print(f\"BERT Calls (Baseline): {bert_calls_baseline}\")\n",
    "print(f\"BERT Calls (Classifier-Gated): {bert_calls_classifier}\")\n",
    "print(f\"Reduction in BERT Calls: {reduction:.2%}\")\n",
    "print(f\"Entity Completion Recall (Y=1 captured): {entity_recall:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
