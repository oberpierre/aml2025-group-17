{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76149ac6",
   "metadata": {},
   "source": [
    "# File for testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64938229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2ad6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load OntoNotes dataset\n",
    "ontonotes = load_dataset(\"conll2012_ontonotesv5\", \"english_v12\")\n",
    "train_data = ontonotes[\"train\"]\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "# Move model to GPU if available\n",
    "device = 'cpu'\n",
    "if torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#  Manually define the NER label list (OntoNotes-style)\n",
    "label_list = [\n",
    "    \"O\", \"B-PERSON\", \"I-PERSON\", \"B-ORG\", \"I-ORG\",\n",
    "    \"B-GPE\", \"I-GPE\", \"B-DATE\", \"I-DATE\",\n",
    "    \"B-CARDINAL\", \"I-CARDINAL\",\n",
    "    \"B-MONEY\", \"I-MONEY\",\n",
    "    \"B-PERCENT\", \"I-PERCENT\",\n",
    "    \"B-TIME\", \"I-TIME\",\n",
    "    \"B-FAC\", \"I-FAC\", \"B-LOC\", \"I-LOC\",\n",
    "    \"B-PRODUCT\", \"I-PRODUCT\",\n",
    "    \"B-WORK_OF_ART\", \"I-WORK_OF_ART\",\n",
    "    \"B-LAW\", \"I-LAW\", \"B-EVENT\", \"I-EVENT\",\n",
    "    \"B-LANGUAGE\", \"I-LANGUAGE\",\n",
    "    \"B-NORP\", \"I-NORP\", \"B-QUANTITY\", \"I-QUANTITY\",\n",
    "    \"B-ORDINAL\", \"I-ORDINAL\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7680ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_entities(words, ner_tags):\n",
    "    \"\"\"\n",
    "    Returns binary labels Y for each timestep, where Y=1 marks the moment an entity is completed.\n",
    "    Supports BIO tagging.\n",
    "    \"\"\"\n",
    "    seen_entities = set()\n",
    "    Y = []\n",
    "\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "\n",
    "    for i in range(1, len(words) + 1):\n",
    "        word = words[i - 1]\n",
    "        tag = ner_tags[i - 1]\n",
    "        label = 0\n",
    "\n",
    "        next_tag = ner_tags[i] if i < len(ner_tags) else 'O'\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            current_entity = [word]\n",
    "            current_type = tag[2:]\n",
    "\n",
    "            # Single-token entity\n",
    "            if not next_tag.startswith(\"I-\") or next_tag[2:] != current_type:\n",
    "                ent_tuple = (tuple(current_entity), current_type)\n",
    "                if ent_tuple not in seen_entities:\n",
    "                    label = 1\n",
    "                    seen_entities.add(ent_tuple)\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "        elif tag.startswith(\"I-\") and current_type == tag[2:]:\n",
    "            current_entity.append(word)\n",
    "\n",
    "            # Last token of multi-token entity\n",
    "            if not next_tag.startswith(\"I-\") or next_tag[2:] != current_type:\n",
    "                ent_tuple = (tuple(current_entity), current_type)\n",
    "                if ent_tuple not in seen_entities:\n",
    "                    label = 1\n",
    "                    seen_entities.add(ent_tuple)\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "        else:\n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "\n",
    "        Y.append(label)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "def process_sentence(words, ner_ids):\n",
    "    ner_tags = [label_list[i] for i in ner_ids]\n",
    "    x_list, y_list = [], []\n",
    "\n",
    "    # Get labels for each timestep using the updated extract_entities function\n",
    "    timestep_labels = extract_entities(words, ner_tags)\n",
    "\n",
    "    for i in range(1, len(words) + 1):\n",
    "        partial = words[:i]\n",
    "        partial_tags = ner_tags[:i]\n",
    "\n",
    "        inputs = tokenizer(\" \".join(partial), return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls = outputs.last_hidden_state[0][0].numpy()\n",
    "\n",
    "        y = timestep_labels[i - 1]\n",
    "\n",
    "        x_list.append(cls)\n",
    "        y_list.append(y)\n",
    "\n",
    "        # print(f\"Timestep {i}\")\n",
    "        # print(f\"Partial sentence: {' '.join(partial)}\")\n",
    "        # print(f\"NER tags so far: {partial_tags}\")\n",
    "        # print(f\"Label Y: {y}\")\n",
    "        # print(\"-\" * 60)\n",
    "\n",
    "    return x_list, y_list\n",
    "\n",
    "def process_sentence_evaluation(words, ner_tags):\n",
    "    \"\"\"\n",
    "    For a sentence and its BIO tags:\n",
    "    Returns one tuple:\n",
    "    (list of unique tags, list of partial sentences (lists of words) where an entity ends)\n",
    "    \"\"\"\n",
    "    # Unique tags in order\n",
    "    seen_tags = set()\n",
    "    unique_tags = []\n",
    "    for tag in ner_tags:\n",
    "        if tag not in seen_tags:\n",
    "            unique_tags.append(tag)\n",
    "            seen_tags.add(tag)\n",
    "\n",
    "    # Get entity ending labels using your existing function\n",
    "    timestep_labels = extract_entities(words, ner_tags)\n",
    "\n",
    "    # Collect only partials where an entity ends\n",
    "    entity_partials = []\n",
    "    for i in range(1, len(words) + 1):\n",
    "        if timestep_labels[i - 1] == 1:\n",
    "            partial = words[:i]\n",
    "            entity_partials.append(partial)\n",
    "\n",
    "    return (unique_tags, entity_partials)\n",
    "\n",
    "\n",
    "def process_sentence_batch(sentences_data, batch_size=32):\n",
    "    \"\"\"\n",
    "    Process multiple sentences in batches for GPU efficiency.\n",
    "    \"\"\"\n",
    "    all_partial_texts = []\n",
    "    all_labels = []\n",
    "    sentence_boundaries = []\n",
    "    \n",
    "    # Prepare all partial sentences and labels\n",
    "    for words, ner_tags in sentences_data:\n",
    "        timestep_labels = extract_entities(words, ner_tags)\n",
    "        sentence_start = len(all_partial_texts)\n",
    "        \n",
    "        for i in range(1, len(words) + 1):\n",
    "            partial = words[:i]\n",
    "            partial_text = \" \".join(partial)\n",
    "            all_partial_texts.append(partial_text)\n",
    "            all_labels.append(timestep_labels[i - 1])\n",
    "        \n",
    "        sentence_boundaries.append((sentence_start, len(all_partial_texts)))\n",
    "    \n",
    "    # Process in batches\n",
    "    X_batch, Y_batch = [], []\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_partial_texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = all_partial_texts[i:i + batch_size]\n",
    "        batch_labels = all_labels[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        X_batch.extend(cls_embeddings)\n",
    "        Y_batch.extend(batch_labels)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        torch.mps.empty_cache() if torch.mps.is_available() else None\n",
    "    \n",
    "    return X_batch, Y_batch\n",
    "\n",
    "def collect_sentence_data(data_subset, max_docs=None):\n",
    "    \"\"\"\n",
    "    Collect all sentence data before processing.\n",
    "    \"\"\"\n",
    "    sentences_data = []\n",
    "    doc_count = 0\n",
    "    \n",
    "    for ex in data_subset:\n",
    "        if max_docs and doc_count >= max_docs:\n",
    "            break\n",
    "        \n",
    "        for sentence in ex[\"sentences\"]:\n",
    "            words = sentence[\"words\"]\n",
    "            ner_ids = sentence[\"named_entities\"]\n",
    "            ner_tags = [label_list[i] for i in ner_ids]\n",
    "            sentences_data.append((words, ner_tags))\n",
    "        \n",
    "        doc_count += 1\n",
    "    \n",
    "    return sentences_data\n",
    "\n",
    "\n",
    "def process_sentence_evaluation_version2(words, ner_pred_ids, label_list):\n",
    "    \"\"\"\n",
    "    words: list of words in sentence\n",
    "    ner_pred_ids: list of predicted label ids from model output (integers)\n",
    "    label_list: list mapping label_id -> BIO tag string, e.g. ['O', 'B-PER', 'I-PER', ...]\n",
    "\n",
    "    Returns:\n",
    "        tuple: (unique_tags, list_of_partial_sentences_where_entity_ends)\n",
    "    \"\"\"\n",
    "    # Convert predicted ids to tags\n",
    "    ner_tags = [label_list[i] for i in ner_pred_ids]\n",
    "\n",
    "    # Unique tags in order\n",
    "    seen_tags = set()\n",
    "    unique_tags = []\n",
    "    for tag in ner_tags:\n",
    "        if tag not in seen_tags:\n",
    "            unique_tags.append(tag)\n",
    "            seen_tags.add(tag)\n",
    "\n",
    "    # Get entity ending labels\n",
    "    timestep_labels = extract_entities(words, ner_tags)\n",
    "\n",
    "    # Collect only partial sentences where entity ends\n",
    "    entity_partials = []\n",
    "    for i in range(1, len(words) + 1):\n",
    "        if timestep_labels[i - 1] == 1:\n",
    "            partial = words[:i]\n",
    "            entity_partials.append(partial)\n",
    "\n",
    "    return (unique_tags, entity_partials)\n",
    "\n",
    "def align_predictions_to_words(predicted_token_ids, word_ids):\n",
    "    word_pred_ids = []\n",
    "    current_word = None\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        if word_id != current_word:\n",
    "            word_pred_ids.append(predicted_token_ids[idx])\n",
    "            current_word = word_id\n",
    "        else:\n",
    "            # Ignore subword token preds for simplicity\n",
    "            pass\n",
    "    return word_pred_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394dfe2",
   "metadata": {},
   "source": [
    "# Testing dataset oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfa1aa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: Dataset({\n",
      "    features: ['document_id', 'sentences'],\n",
      "    num_rows: 1200\n",
      "})\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.']\n",
      "{'part_id': 0, 'words': ['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.'], 'pos_tags': [9, 33, 5, 31, 41, 33, 43, 18, 18, 14, 19, 19, 28, 8], 'parse_tree': '(TOP(S (: --) (ADVP (RB basically) ) (, ,) (NP (PRP it) )(VP (VBD was) (ADVP (RB unanimously) )(VP (VBN agreed) (PP (IN upon) )(PP (IN by) (NP (DT the)  (JJ various)  (JJ relevant)  (NNS parties) )))) (. .) ))', 'predicate_lemmas': [None, None, None, None, 'be', None, 'agree', None, None, None, None, None, None, None], 'predicate_framenet_ids': [None, None, None, None, '03', None, '01', None, None, None, None, None, None, None], 'word_senses': [None, None, None, None, None, None, 1.0, None, None, None, None, None, None, None], 'speaker': 'speaker#1', 'named_entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'srl_frames': [{'verb': 'was', 'frames': ['O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'agreed', 'frames': ['O', 'B-ARGM-ADV', 'O', 'O', 'O', 'B-ARGM-MNR', 'B-V', 'B-ARG1', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'O']}], 'coref_spans': []}\n"
     ]
    }
   ],
   "source": [
    "test_data = ontonotes[\"test\"]\n",
    "print(f\"test_data: {test_data}\")\n",
    "print((test_data[\"sentences\"][0][0][\"words\"]))\n",
    "print(test_data[\"sentences\"][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97f8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents:   0%|          | 2/1200 [01:27<14:32:42, 43.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(words, is_split_into_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ner_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# [batch_size, seq_len, num_labels]\u001b[39;00m\n\u001b[0;32m     13\u001b[0m predicted_token_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1677\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1675\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1677\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1678\u001b[0m     input_ids,\n\u001b[0;32m   1679\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1680\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1681\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1682\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1683\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1684\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1685\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1686\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1687\u001b[0m )\n\u001b[0;32m   1689\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1691\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    994\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    997\u001b[0m     embedding_output,\n\u001b[0;32m    998\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    999\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1000\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1001\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1002\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1003\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1004\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1005\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1006\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1007\u001b[0m )\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:651\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    648\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    649\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    652\u001b[0m     hidden_states,\n\u001b[0;32m    653\u001b[0m     attention_mask,\n\u001b[0;32m    654\u001b[0m     layer_head_mask,\n\u001b[0;32m    655\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    657\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    658\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    659\u001b[0m )\n\u001b[0;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:595\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    592\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    593\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 595\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    597\u001b[0m )\n\u001b[0;32m    598\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\pytorch_utils.py:250\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:608\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    607\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 608\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bert\\modeling_bert.py:520\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 520\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    521\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    522\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for row in tqdm(test_data, desc=\"Documents\"):\n",
    "    for sentence_dict in row[\"sentences\"]:\n",
    "        words = sentence_dict[\"words\"]\n",
    "\n",
    "        inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = ner_model(**inputs)\n",
    "            logits = outputs.logits  # [batch_size, seq_len, num_labels]\n",
    "\n",
    "        predicted_token_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "        word_ids = inputs.word_ids()\n",
    "        word_level_pred_ids = align_predictions_to_words(predicted_token_ids, word_ids)\n",
    "\n",
    "        label_list = [ner_model.config.id2label[i] for i in range(len(ner_model.config.id2label))]\n",
    "\n",
    "        result = process_sentence_evaluation_version2(words, word_level_pred_ids, label_list)\n",
    "\n",
    "        results.append(result)\n",
    "        \n",
    "results_array = np.array(results, dtype=object)\n",
    "\n",
    "# Save compressed npz\n",
    "np.savez_compressed(\"first_model_evaluation_data.npz\", data=results_array)\n",
    "print(f\"Processed {len(results)} sentences and saved results to first_model_evaluation_data.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18bb6c1",
   "metadata": {},
   "source": [
    "# dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb24275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC'], [['Barack', 'Obama'], ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii'], ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', 'and', 'lives', 'in', 'Washington', 'DC']])\n"
     ]
    }
   ],
   "source": [
    "words = [\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\", \"and\", \"lives\", \"in\", \"Washington\", \"DC\"]\n",
    "ner_tags = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\"]\n",
    "\n",
    "\n",
    "print(process_sentence_evaluation(words, ner_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7429c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags in sentence: ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
      "Partial sentences where entities end:\n",
      "['Barack', 'Obama']\n",
      "['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii']\n"
     ]
    }
   ],
   "source": [
    "label_list = model.config.id2label  # dict {id: label}\n",
    "label_list = [label_list[i] for i in range(len(label_list))]  # list ordered by id\n",
    "words = [\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\"]\n",
    "inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ner_model(**inputs)\n",
    "    logits = outputs.logits  # shape: [batch_size, seq_len, num_labels]\n",
    "    \n",
    "predicted_token_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "word_ids = inputs.word_ids()  # list with word index for each token\n",
    "word_pred_ids = []\n",
    "current_word = None\n",
    "for idx, word_id in enumerate(word_ids):\n",
    "    if word_id is None:\n",
    "        continue\n",
    "    if word_id != current_word:\n",
    "        # start of a new word\n",
    "        word_pred_ids.append(predicted_token_ids[idx])\n",
    "        current_word = word_id\n",
    "    else:\n",
    "        # For subword tokens of the same word, you can choose to keep first or most confident prediction\n",
    "        # Here we ignore subword predictions for simplicity\n",
    "        pass\n",
    "\n",
    "# Now call your function\n",
    "result = process_sentence_evaluation_version2(words, word_pred_ids, label_list)\n",
    "\n",
    "print(\"Unique tags in sentence:\", result[0])\n",
    "print(\"Partial sentences where entities end:\")\n",
    "for partial in result[1]:\n",
    "    print(partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d704b64",
   "metadata": {},
   "source": [
    "# Not used anymore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e679a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with first 10 documents...\n",
      "Collected 5445 sentences from 10 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 285/285 [02:25<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 72860 samples from test run.\n",
      "Positive samples: 4591\n",
      "Negative samples: 68269\n",
      "Saved test results to ontonotes_embeddings_test.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with smaller subset first\n",
    "print(\"Testing with first 10 documents...\")\n",
    "sentences_data = collect_sentence_data(train_data, max_docs=10)\n",
    "print(f\"Collected {len(sentences_data)} sentences from 10 documents\")\n",
    "\n",
    "# Process in batches\n",
    "X_all, Y_all = process_sentence_batch(sentences_data, batch_size=256)\n",
    "\n",
    "print(f\"Collected {len(X_all)} samples from test run.\")\n",
    "print(f\"Positive samples: {sum(Y_all)}\")\n",
    "print(f\"Negative samples: {len(Y_all) - sum(Y_all)}\")\n",
    "\n",
    "# Save test results\n",
    "X_array = np.array(X_all)\n",
    "Y_array = np.array(Y_all)\n",
    "np.savez(\"ontonotes_embeddings_test.npz\", X=X_array, Y=Y_array)\n",
    "print(\"Saved test results to ontonotes_embeddings_test.npz\")\n",
    "\n",
    "# Clean up memory\n",
    "del X_all, Y_all, X_array, Y_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01af176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing documents 0 to 999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1970/1970 [19:52<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 504191\n",
      "\n",
      "Processing documents 1000 to 1999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1960/1960 [20:23<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 1005878\n",
      "\n",
      "Processing documents 2000 to 2999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2246/2246 [21:46<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 1580775\n",
      "\n",
      "Processing documents 3000 to 3999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1792/1792 [19:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2039335\n",
      "\n",
      "Processing documents 4000 to 4999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 92/92 [01:07<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2062809\n",
      "Saving intermediate results...\n",
      "\n",
      "Processing documents 5000 to 5999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 98/98 [01:17<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2087771\n",
      "\n",
      "Processing documents 6000 to 6999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 93/93 [01:20<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2111389\n",
      "\n",
      "Processing documents 7000 to 7999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 104/104 [01:37<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2137962\n",
      "\n",
      "Processing documents 8000 to 8999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 98/98 [01:23<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2162815\n",
      "\n",
      "Processing documents 9000 to 9999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 96/96 [01:41<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2187200\n",
      "Saving intermediate results...\n",
      "\n",
      "Processing documents 10000 to 10538...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 54/54 [01:02<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2200865\n",
      "\n",
      "Final: Collected 2200865 samples.\n",
      "Positive samples: 125904\n",
      "Negative samples: 2074961\n",
      "Saved full results to ontonotes_embeddings_full.npz\n"
     ]
    }
   ],
   "source": [
    "# Process full dataset in chunks to avoid memory issues\n",
    "chunk_size = 1000  # Process 1000 documents at a time\n",
    "total_docs = len(train_data)\n",
    "X_all, Y_all = [], []\n",
    "\n",
    "for chunk_start in range(0, total_docs, chunk_size):\n",
    "    chunk_end = min(chunk_start + chunk_size, total_docs)\n",
    "    print(f\"\\nProcessing documents {chunk_start} to {chunk_end-1}...\")\n",
    "    \n",
    "    # Get chunk data\n",
    "    chunk_data = train_data.select(range(chunk_start, chunk_end))\n",
    "    sentences_data = collect_sentence_data(chunk_data)\n",
    "    \n",
    "    # Process chunk\n",
    "    X_chunk, Y_chunk = process_sentence_batch(sentences_data, batch_size=256)\n",
    "    \n",
    "    X_all.extend(X_chunk)\n",
    "    Y_all.extend(Y_chunk)\n",
    "    \n",
    "    print(f\"Chunk complete. Total samples so far: {len(X_all)}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del X_chunk, Y_chunk, sentences_data\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save intermediate results every few chunks\n",
    "    if (chunk_start // chunk_size) % 5 == 4:\n",
    "        print(\"Saving intermediate results...\")\n",
    "        X_array = np.array(X_all)\n",
    "        Y_array = np.array(Y_all)\n",
    "        np.savez(f\"ontonotes_embeddings_intermediate_{chunk_start}.npz\", X=X_array, Y=Y_array)\n",
    "        del X_array, Y_array\n",
    "\n",
    "print(f\"\\nFinal: Collected {len(X_all)} samples.\")\n",
    "print(f\"Positive samples: {sum(Y_all)}\")\n",
    "print(f\"Negative samples: {len(Y_all) - sum(Y_all)}\")\n",
    "\n",
    "# Save final results\n",
    "X_array = np.array(X_all)\n",
    "Y_array = np.array(Y_all)\n",
    "np.savez(\"ontonotes_embeddings_full.npz\", X=X_array, Y=Y_array)\n",
    "print(\"Saved full results to ontonotes_embeddings_full.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b834327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ontonotes_embeddings_*.npz are too large for git, therefore it's available online at https://drive.google.com/drive/folders/1ykTaDLdHIEmZQYN0b1Hr9hkOYjgMshSa?usp=sharing\n"
     ]
    }
   ],
   "source": [
    "print(\"ontonotes_embeddings_*.npz are too large for git, therefore it's available online at https://drive.google.com/drive/folders/1ykTaDLdHIEmZQYN0b1Hr9hkOYjgMshSa?usp=sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc73b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
