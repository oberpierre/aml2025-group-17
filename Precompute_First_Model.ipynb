{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76149ac6",
   "metadata": {},
   "source": [
    "# File for testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64938229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel,AutoModelForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2ad6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load OntoNotes dataset\n",
    "ontonotes = load_dataset(\"conll2012_ontonotesv5\", \"english_v12\")\n",
    "train_data = ontonotes[\"train\"]\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "# Move model to GPU if available\n",
    "device = 'cpu'\n",
    "if torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#  Manually define the NER label list (OntoNotes-style)\n",
    "label_list = [\n",
    "    \"O\", \"B-PERSON\", \"I-PERSON\", \"B-ORG\", \"I-ORG\",\n",
    "    \"B-GPE\", \"I-GPE\", \"B-DATE\", \"I-DATE\",\n",
    "    \"B-CARDINAL\", \"I-CARDINAL\",\n",
    "    \"B-MONEY\", \"I-MONEY\",\n",
    "    \"B-PERCENT\", \"I-PERCENT\",\n",
    "    \"B-TIME\", \"I-TIME\",\n",
    "    \"B-FAC\", \"I-FAC\", \"B-LOC\", \"I-LOC\",\n",
    "    \"B-PRODUCT\", \"I-PRODUCT\",\n",
    "    \"B-WORK_OF_ART\", \"I-WORK_OF_ART\",\n",
    "    \"B-LAW\", \"I-LAW\", \"B-EVENT\", \"I-EVENT\",\n",
    "    \"B-LANGUAGE\", \"I-LANGUAGE\",\n",
    "    \"B-NORP\", \"I-NORP\", \"B-QUANTITY\", \"I-QUANTITY\",\n",
    "    \"B-ORDINAL\", \"I-ORDINAL\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7680ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_entities(words, ner_tags):\n",
    "    \"\"\"\n",
    "    Returns binary labels Y for each timestep, where Y=1 marks the moment an entity is completed.\n",
    "    Supports BIO tagging.\n",
    "    \"\"\"\n",
    "    seen_entities = set()\n",
    "    Y = []\n",
    "\n",
    "    current_entity = []\n",
    "    current_type = None\n",
    "\n",
    "    for i in range(1, len(words) + 1):\n",
    "        word = words[i - 1]\n",
    "        tag = ner_tags[i - 1]\n",
    "        label = 0\n",
    "\n",
    "        next_tag = ner_tags[i] if i < len(ner_tags) else 'O'\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            current_entity = [word]\n",
    "            current_type = tag[2:]\n",
    "\n",
    "            # Single-token entity\n",
    "            if not next_tag.startswith(\"I-\") or next_tag[2:] != current_type:\n",
    "                ent_tuple = (tuple(current_entity), current_type)\n",
    "                if ent_tuple not in seen_entities:\n",
    "                    label = 1\n",
    "                    seen_entities.add(ent_tuple)\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "        elif tag.startswith(\"I-\") and current_type == tag[2:]:\n",
    "            current_entity.append(word)\n",
    "\n",
    "            # Last token of multi-token entity\n",
    "            if not next_tag.startswith(\"I-\") or next_tag[2:] != current_type:\n",
    "                ent_tuple = (tuple(current_entity), current_type)\n",
    "                if ent_tuple not in seen_entities:\n",
    "                    label = 1\n",
    "                    seen_entities.add(ent_tuple)\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "\n",
    "        else:\n",
    "            current_entity = []\n",
    "            current_type = None\n",
    "\n",
    "        Y.append(label)\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "\n",
    "def process_sentence(words, ner_ids):\n",
    "    ner_tags = [label_list[i] for i in ner_ids]\n",
    "    x_list, y_list = [], []\n",
    "\n",
    "    # Get labels for each timestep using the updated extract_entities function\n",
    "    timestep_labels = extract_entities(words, ner_tags)\n",
    "\n",
    "    for i in range(1, len(words) + 1):\n",
    "        partial = words[:i]\n",
    "        partial_tags = ner_tags[:i]\n",
    "\n",
    "        inputs = tokenizer(\" \".join(partial), return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls = outputs.last_hidden_state[0][0].numpy()\n",
    "\n",
    "        y = timestep_labels[i - 1]\n",
    "\n",
    "        x_list.append(cls)\n",
    "        y_list.append(y)\n",
    "\n",
    "        # print(f\"Timestep {i}\")\n",
    "        # print(f\"Partial sentence: {' '.join(partial)}\")\n",
    "        # print(f\"NER tags so far: {partial_tags}\")\n",
    "        # print(f\"Label Y: {y}\")\n",
    "        # print(\"-\" * 60)\n",
    "\n",
    "    return x_list, y_list\n",
    "\n",
    "def process_sentence_evaluation(words, ner_tags):\n",
    "    \"\"\"\n",
    "    For a sentence and its BIO tags:\n",
    "    Returns one tuple:\n",
    "    (list of unique tags, list of partial sentences (lists of words) where an entity ends)\n",
    "    \"\"\"\n",
    "    # Unique tags in order\n",
    "    seen_tags = set()\n",
    "    unique_tags = []\n",
    "    for tag in ner_tags:\n",
    "        if tag not in seen_tags:\n",
    "            unique_tags.append(tag)\n",
    "            seen_tags.add(tag)\n",
    "\n",
    "    # Get entity ending labels using your existing function\n",
    "    timestep_labels = extract_entities(words, ner_tags)\n",
    "\n",
    "    # Collect only partials where an entity ends\n",
    "    entity_partials = []\n",
    "    for i in range(1, len(words) + 1):\n",
    "        if timestep_labels[i - 1] == 1:\n",
    "            partial = words[:i]\n",
    "            entity_partials.append(partial)\n",
    "\n",
    "    return (unique_tags, entity_partials)\n",
    "\n",
    "\n",
    "def process_sentence_batch(sentences_data, batch_size=32):\n",
    "    \"\"\"\n",
    "    Process multiple sentences in batches for GPU efficiency.\n",
    "    \"\"\"\n",
    "    all_partial_texts = []\n",
    "    all_labels = []\n",
    "    sentence_boundaries = []\n",
    "    \n",
    "    # Prepare all partial sentences and labels\n",
    "    for words, ner_tags in sentences_data:\n",
    "        timestep_labels = extract_entities(words, ner_tags)\n",
    "        sentence_start = len(all_partial_texts)\n",
    "        \n",
    "        for i in range(1, len(words) + 1):\n",
    "            partial = words[:i]\n",
    "            partial_text = \" \".join(partial)\n",
    "            all_partial_texts.append(partial_text)\n",
    "            all_labels.append(timestep_labels[i - 1])\n",
    "        \n",
    "        sentence_boundaries.append((sentence_start, len(all_partial_texts)))\n",
    "    \n",
    "    # Process in batches\n",
    "    X_batch, Y_batch = [], []\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_partial_texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = all_partial_texts[i:i + batch_size]\n",
    "        batch_labels = all_labels[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        X_batch.extend(cls_embeddings)\n",
    "        Y_batch.extend(batch_labels)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        torch.mps.empty_cache() if torch.mps.is_available() else None\n",
    "    \n",
    "    return X_batch, Y_batch\n",
    "\n",
    "def collect_sentence_data(data_subset, max_docs=None):\n",
    "    \"\"\"\n",
    "    Collect all sentence data before processing.\n",
    "    \"\"\"\n",
    "    sentences_data = []\n",
    "    doc_count = 0\n",
    "    \n",
    "    for ex in data_subset:\n",
    "        if max_docs and doc_count >= max_docs:\n",
    "            break\n",
    "        \n",
    "        for sentence in ex[\"sentences\"]:\n",
    "            words = sentence[\"words\"]\n",
    "            ner_ids = sentence[\"named_entities\"]\n",
    "            ner_tags = [label_list[i] for i in ner_ids]\n",
    "            sentences_data.append((words, ner_tags))\n",
    "        \n",
    "        doc_count += 1\n",
    "    \n",
    "    return sentences_data\n",
    "\n",
    "\n",
    "def process_sentence_evaluation_version2(words, ner_pred_ids, label_list):\n",
    "    \"\"\"\n",
    "    words: list of words in sentence\n",
    "    ner_pred_ids: list of predicted label ids from model output (integers)\n",
    "    label_list: list mapping label_id -> BIO tag string, e.g. ['O', 'B-PER', 'I-PER', ...]\n",
    "\n",
    "    Returns:\n",
    "        tuple: (unique_tags, list_of_partial_sentences_where_entity_ends)\n",
    "    \"\"\"\n",
    "    # Convert predicted ids to tags\n",
    "    ner_tags = [label_list[i] for i in ner_pred_ids]\n",
    "\n",
    "    # Unique tags in order\n",
    "    seen_tags = set()\n",
    "    unique_tags = []\n",
    "    for tag in ner_tags:\n",
    "        if tag not in seen_tags:\n",
    "            unique_tags.append(tag)\n",
    "            seen_tags.add(tag)\n",
    "\n",
    "    # Get entity ending labels\n",
    "    timestep_labels = extract_entities(words, ner_tags)\n",
    "\n",
    "    # Collect only partial sentences where entity ends\n",
    "    entity_partials = []\n",
    "    for i in range(1, len(words) + 1):\n",
    "        if timestep_labels[i - 1] == 1:\n",
    "            partial = words[:i]\n",
    "            entity_partials.append(partial)\n",
    "\n",
    "    return (unique_tags, entity_partials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394dfe2",
   "metadata": {},
   "source": [
    "# Testing dataset oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfa1aa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: Dataset({\n",
      "    features: ['document_id', 'sentences'],\n",
      "    num_rows: 1200\n",
      "})\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.']\n",
      "{'part_id': 0, 'words': ['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.'], 'pos_tags': [9, 33, 5, 31, 41, 33, 43, 18, 18, 14, 19, 19, 28, 8], 'parse_tree': '(TOP(S (: --) (ADVP (RB basically) ) (, ,) (NP (PRP it) )(VP (VBD was) (ADVP (RB unanimously) )(VP (VBN agreed) (PP (IN upon) )(PP (IN by) (NP (DT the)  (JJ various)  (JJ relevant)  (NNS parties) )))) (. .) ))', 'predicate_lemmas': [None, None, None, None, 'be', None, 'agree', None, None, None, None, None, None, None], 'predicate_framenet_ids': [None, None, None, None, '03', None, '01', None, None, None, None, None, None, None], 'word_senses': [None, None, None, None, None, None, 1.0, None, None, None, None, None, None, None], 'speaker': 'speaker#1', 'named_entities': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'srl_frames': [{'verb': 'was', 'frames': ['O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'agreed', 'frames': ['O', 'B-ARGM-ADV', 'O', 'O', 'O', 'B-ARGM-MNR', 'B-V', 'B-ARG1', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'O']}], 'coref_spans': []}\n"
     ]
    }
   ],
   "source": [
    "test_data = ontonotes[\"test\"]\n",
    "print(f\"test_data: {test_data}\")\n",
    "print((test_data[\"sentences\"][0][0][\"words\"]))\n",
    "print(test_data[\"sentences\"][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb24275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC'], [['Barack', 'Obama'], ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii'], ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', 'and', 'lives', 'in', 'Washington', 'DC']])\n"
     ]
    }
   ],
   "source": [
    "words = [\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\", \"and\", \"lives\", \"in\", \"Washington\", \"DC\"]\n",
    "ner_tags = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\"]\n",
    "\n",
    "\n",
    "print(process_sentence_evaluation(words, ner_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7429c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags in sentence: ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
      "Partial sentences where entities end:\n",
      "['Barack', 'Obama']\n",
      "['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii']\n"
     ]
    }
   ],
   "source": [
    "label_list = model.config.id2label  # dict {id: label}\n",
    "label_list = [label_list[i] for i in range(len(label_list))]  # list ordered by id\n",
    "words = [\"Barack\", \"Obama\", \"was\", \"born\", \"in\", \"Hawaii\"]\n",
    "inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ner_model(**inputs)\n",
    "    logits = outputs.logits  # shape: [batch_size, seq_len, num_labels]\n",
    "    \n",
    "predicted_token_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "word_ids = inputs.word_ids()  # list with word index for each token\n",
    "word_pred_ids = []\n",
    "current_word = None\n",
    "for idx, word_id in enumerate(word_ids):\n",
    "    if word_id is None:\n",
    "        continue\n",
    "    if word_id != current_word:\n",
    "        # start of a new word\n",
    "        word_pred_ids.append(predicted_token_ids[idx])\n",
    "        current_word = word_id\n",
    "    else:\n",
    "        # For subword tokens of the same word, you can choose to keep first or most confident prediction\n",
    "        # Here we ignore subword predictions for simplicity\n",
    "        pass\n",
    "\n",
    "# Now call your function\n",
    "result = process_sentence_evaluation_version2(words, word_pred_ids, label_list)\n",
    "\n",
    "print(\"Unique tags in sentence:\", result[0])\n",
    "print(\"Partial sentences where entities end:\")\n",
    "for partial in result[1]:\n",
    "    print(partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d704b64",
   "metadata": {},
   "source": [
    "# Deprecated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e679a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with first 10 documents...\n",
      "Collected 5445 sentences from 10 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 285/285 [02:25<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 72860 samples from test run.\n",
      "Positive samples: 4591\n",
      "Negative samples: 68269\n",
      "Saved test results to ontonotes_embeddings_test.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with smaller subset first\n",
    "print(\"Testing with first 10 documents...\")\n",
    "sentences_data = collect_sentence_data(train_data, max_docs=10)\n",
    "print(f\"Collected {len(sentences_data)} sentences from 10 documents\")\n",
    "\n",
    "# Process in batches\n",
    "X_all, Y_all = process_sentence_batch(sentences_data, batch_size=256)\n",
    "\n",
    "print(f\"Collected {len(X_all)} samples from test run.\")\n",
    "print(f\"Positive samples: {sum(Y_all)}\")\n",
    "print(f\"Negative samples: {len(Y_all) - sum(Y_all)}\")\n",
    "\n",
    "# Save test results\n",
    "X_array = np.array(X_all)\n",
    "Y_array = np.array(Y_all)\n",
    "np.savez(\"ontonotes_embeddings_test.npz\", X=X_array, Y=Y_array)\n",
    "print(\"Saved test results to ontonotes_embeddings_test.npz\")\n",
    "\n",
    "# Clean up memory\n",
    "del X_all, Y_all, X_array, Y_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01af176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing documents 0 to 999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1970/1970 [19:52<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 504191\n",
      "\n",
      "Processing documents 1000 to 1999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1960/1960 [20:23<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 1005878\n",
      "\n",
      "Processing documents 2000 to 2999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2246/2246 [21:46<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 1580775\n",
      "\n",
      "Processing documents 3000 to 3999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1792/1792 [19:38<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2039335\n",
      "\n",
      "Processing documents 4000 to 4999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 92/92 [01:07<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2062809\n",
      "Saving intermediate results...\n",
      "\n",
      "Processing documents 5000 to 5999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 98/98 [01:17<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2087771\n",
      "\n",
      "Processing documents 6000 to 6999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 93/93 [01:20<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2111389\n",
      "\n",
      "Processing documents 7000 to 7999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 104/104 [01:37<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2137962\n",
      "\n",
      "Processing documents 8000 to 8999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 98/98 [01:23<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2162815\n",
      "\n",
      "Processing documents 9000 to 9999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 96/96 [01:41<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2187200\n",
      "Saving intermediate results...\n",
      "\n",
      "Processing documents 10000 to 10538...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 54/54 [01:02<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk complete. Total samples so far: 2200865\n",
      "\n",
      "Final: Collected 2200865 samples.\n",
      "Positive samples: 125904\n",
      "Negative samples: 2074961\n",
      "Saved full results to ontonotes_embeddings_full.npz\n"
     ]
    }
   ],
   "source": [
    "# Process full dataset in chunks to avoid memory issues\n",
    "chunk_size = 1000  # Process 1000 documents at a time\n",
    "total_docs = len(train_data)\n",
    "X_all, Y_all = [], []\n",
    "\n",
    "for chunk_start in range(0, total_docs, chunk_size):\n",
    "    chunk_end = min(chunk_start + chunk_size, total_docs)\n",
    "    print(f\"\\nProcessing documents {chunk_start} to {chunk_end-1}...\")\n",
    "    \n",
    "    # Get chunk data\n",
    "    chunk_data = train_data.select(range(chunk_start, chunk_end))\n",
    "    sentences_data = collect_sentence_data(chunk_data)\n",
    "    \n",
    "    # Process chunk\n",
    "    X_chunk, Y_chunk = process_sentence_batch(sentences_data, batch_size=256)\n",
    "    \n",
    "    X_all.extend(X_chunk)\n",
    "    Y_all.extend(Y_chunk)\n",
    "    \n",
    "    print(f\"Chunk complete. Total samples so far: {len(X_all)}\")\n",
    "    \n",
    "    # Clear memory\n",
    "    del X_chunk, Y_chunk, sentences_data\n",
    "    gc.collect()\n",
    "    \n",
    "    # Save intermediate results every few chunks\n",
    "    if (chunk_start // chunk_size) % 5 == 4:\n",
    "        print(\"Saving intermediate results...\")\n",
    "        X_array = np.array(X_all)\n",
    "        Y_array = np.array(Y_all)\n",
    "        np.savez(f\"ontonotes_embeddings_intermediate_{chunk_start}.npz\", X=X_array, Y=Y_array)\n",
    "        del X_array, Y_array\n",
    "\n",
    "print(f\"\\nFinal: Collected {len(X_all)} samples.\")\n",
    "print(f\"Positive samples: {sum(Y_all)}\")\n",
    "print(f\"Negative samples: {len(Y_all) - sum(Y_all)}\")\n",
    "\n",
    "# Save final results\n",
    "X_array = np.array(X_all)\n",
    "Y_array = np.array(Y_all)\n",
    "np.savez(\"ontonotes_embeddings_full.npz\", X=X_array, Y=Y_array)\n",
    "print(\"Saved full results to ontonotes_embeddings_full.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b834327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ontonotes_embeddings_*.npz are too large for git, therefore it's available online at https://drive.google.com/drive/folders/1ykTaDLdHIEmZQYN0b1Hr9hkOYjgMshSa?usp=sharing\n"
     ]
    }
   ],
   "source": [
    "print(\"ontonotes_embeddings_*.npz are too large for git, therefore it's available online at https://drive.google.com/drive/folders/1ykTaDLdHIEmZQYN0b1Hr9hkOYjgMshSa?usp=sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc73b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
