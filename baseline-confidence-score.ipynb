{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d9b6ce",
   "metadata": {},
   "source": [
    "# Confidence Score Baseline\n",
    "\n",
    "This notebook implements the confidence score (approach 1) baseline for near real-time Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fbfba9",
   "metadata": {},
   "source": [
    "## 1. Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f81fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e084192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "350ceece",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src\")\n",
    "from utils import convert_ids_to_bio, convert_predictions\n",
    "from confidence_model import confidence_model\n",
    "from metrics import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75ef19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import metrics\n",
    "reload(metrics)\n",
    "from metrics import Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fe5f8",
   "metadata": {},
   "source": [
    "## 2. Load OntoNotes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ff89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with splits: dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "# Load the English portion of OntoNotes 5.0\n",
    "ontonotes = load_dataset(\n",
    "    \"conll2012_ontonotesv5\",\n",
    "    \"english_v12\",\n",
    "    cache_dir=\"./dataset/ontonotes\",\n",
    ")\n",
    "print(f\"Dataset loaded with splits: {ontonotes.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459811a1",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Here we create all window prefixes and pre-compute the CLS token over the OntoNotes test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d919aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1200/1200 [00:00<00:00, 1336.49doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prefixes created: 230118\n",
      "Example prefix: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['--']\n",
      "['--', 'basically']\n",
      "['--', 'basically', ',']\n",
      "['--', 'basically', ',', 'it']\n",
      "['--', 'basically', ',', 'it', 'was']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prefixes = []\n",
    "prefix_count = 0\n",
    "\n",
    "# Iterate through the test split\n",
    "for doc in tqdm(ontonotes[\"test\"], desc=\"Processing documents\", unit=\"doc\"):\n",
    "    # Fix: Sometimes doc['sentences'] is a list of lists, so we need to flatten it\n",
    "    if isinstance(doc['sentences'], list) and isinstance(doc['sentences'][0], list):\n",
    "        doc['sentences'] = [sentence for sublist in doc['sentences'] for sentence in sublist]\n",
    "    for sentence in doc['sentences']:\n",
    "        sentence_prefixes = []\n",
    "        curr_prefix = []\n",
    "        for word in sentence['words']:\n",
    "            curr_prefix.append(word)\n",
    "            prefix_count += 1\n",
    "            sentence_prefixes.append(curr_prefix.copy())\n",
    "\n",
    "        true_bio = convert_ids_to_bio(sentence['named_entities'])\n",
    "\n",
    "        # Store the current prefix and BIO tags\n",
    "        prefixes.append((true_bio, sentence_prefixes))\n",
    "\n",
    "print(f\"Total prefixes created: {prefix_count}\")\n",
    "print(f\"Example prefix: {prefixes[0][0]}\\n{\"\\n\".join([str(x) for x in prefixes[0][1]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96340bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification, pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model = AutoModel.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "472645ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prefixes to process: 230118\n"
     ]
    }
   ],
   "source": [
    "all_prefixes = []\n",
    "for _, sentence_prefixes in prefixes:\n",
    "    all_prefixes.extend(sentence_prefixes)\n",
    "\n",
    "print(f\"Total prefixes to process: {len(all_prefixes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ddbe56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing CLS token for windows: 100%|██████████| 1798/1798 [07:45<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (230118, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate all embeddings for the prefixes in batches\n",
    "\n",
    "batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(all_prefixes), batch_size), desc=\"Computing CLS token for prefixes\"):\n",
    "    batch = all_prefixes[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "    embeddings.append(cls_token.cpu().numpy())\n",
    "\n",
    "# Unbatch the embeddings\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bbfdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"data/ontonotes_embeddings_test_all_prefixes.npz\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c18b3954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (230118, 768)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"data/ontonotes_embeddings_test_all_prefixes.npz\")\n",
    "embeddings = data[\"embeddings\"]\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9bf9b",
   "metadata": {},
   "source": [
    "## 4. Implementing confidence score baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d41621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence_model(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_model = confidence_model()\n",
    "conf_model.load_state_dict(torch.load(\"models/confidence_model.pth\", weights_only=True))\n",
    "conf_model.to(device)\n",
    "conf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5c96020",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Determining invocation times: 100%|██████████| 1798/1798 [00:01<00:00, 1078.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictions: (230118, 1)\n",
      "Number of invocations: 14733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "threshold = 0.8\n",
    "y_hat = []\n",
    "for i in tqdm(range(0, len(embeddings), batch_size), desc=\"Determining invocation times\"):\n",
    "    cls_token = torch.tensor(embeddings[i:i + batch_size]).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = conf_model(cls_token)\n",
    "    y_hat_batch = torch.where(torch.sigmoid(logits) > threshold, 1, 0)\n",
    "\n",
    "    y_hat.append(y_hat_batch.cpu().numpy())\n",
    "\n",
    "# unbatch of y_hat\n",
    "y_hat = np.concatenate(y_hat, axis=0)\n",
    "print(f\"Shape of predictions: {y_hat.shape}\")\n",
    "print(f\"Number of invocations: {np.sum(y_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e09c85",
   "metadata": {},
   "source": [
    "## 5. Evaludating sliding window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da340dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner_classifier = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_classifier.to(device)\n",
    "ner_classifier.eval()\n",
    "ner_pipeline = pipeline(\"ner\", model=ner_classifier, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43ba39a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12217/12217 [02:11<00:00, 93.26it/s] \n"
     ]
    }
   ],
   "source": [
    "confidence_metrics = Metrics()\n",
    "ix = 0\n",
    "\n",
    "for true_bio, sentence_prefixes in tqdm(prefixes):\n",
    "    invocations = []\n",
    "    for prefix in sentence_prefixes:\n",
    "        if y_hat[ix] == 1:\n",
    "            results = ner_pipeline(\" \".join(prefix))\n",
    "            invocations.append(convert_predictions(prefix, results))\n",
    "        ix += 1\n",
    "\n",
    "    confidence_metrics.evaluate_metrics(([], true_bio), invocations)\n",
    "\n",
    "confidence_metrics.save_metrics(\"baselines/confidence_score.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fc02f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      "Total NER invocations: 14733\n",
      "Avg TTFD: 1.46\n",
      "FPR@FNR: 0.0858@0.7293\n",
      "Entity Type          TP         TN         FP (#B-/I-MISC)      FN        \n",
      "----------------------------------------------------------------------\n",
      "O                    N/A        52632      1463                 N/A       \n",
      "B-PERSON             1198       N/A        81                   855       \n",
      "I-PERSON             736        N/A        62                   714       \n",
      "B-NORP               0          N/A        681 (663/3)          309       \n",
      "I-NORP               0          N/A        107 (37/62)          55        \n",
      "B-FAC                0          N/A        73 (2/0)             76        \n",
      "I-FAC                0          N/A        163 (3/6)            80        \n",
      "B-ORG                933        N/A        106                  963       \n",
      "I-ORG                1270       N/A        347                  1086      \n",
      "B-GPE                0          N/A        1852 (36/1)          694       \n",
      "I-GPE                0          N/A        489 (3/5)            228       \n",
      "B-LOC                99         N/A        5                    111       \n",
      "I-LOC                84         N/A        48                   70        \n",
      "B-PRODUCT            0          N/A        39 (18/4)            51        \n",
      "I-PRODUCT            0          N/A        37 (13/18)           33        \n",
      "B-DATE               0          N/A        6 (5/0)              1781      \n",
      "I-DATE               0          N/A        11 (5/6)             2186      \n",
      "B-TIME               0          N/A        0                    225       \n",
      "I-TIME               0          N/A        0                    271       \n",
      "B-PERCENT            0          N/A        0                    408       \n",
      "I-PERCENT            0          N/A        0                    619       \n",
      "B-MONEY              0          N/A        0                    355       \n",
      "I-MONEY              0          N/A        57 (57/0)            718       \n",
      "B-QUANTITY           0          N/A        1 (0/1)              152       \n",
      "I-QUANTITY           0          N/A        0                    262       \n",
      "B-ORDINAL            0          N/A        1 (1/0)              206       \n",
      "I-ORDINAL            0          N/A        2                    4         \n",
      "B-CARDINAL           0          N/A        8 (3/3)              997       \n",
      "I-CARDINAL           0          N/A        0                    370       \n",
      "B-EVENT              0          N/A        26 (19/0)            59        \n",
      "I-EVENT              0          N/A        82 (19/58)           83        \n",
      "B-WORK_OF_ART        0          N/A        55 (39/0)            114       \n",
      "I-WORK_OF_ART        0          N/A        128 (20/67)          219       \n",
      "B-LAW                0          N/A        5 (5/0)              39        \n",
      "I-LAW                0          N/A        41 (10/24)           77        \n",
      "B-LANGUAGE           0          N/A        16 (15/1)            6         \n",
      "I-LANGUAGE           0          N/A        0                    0         \n",
      "Total                4320       52632      5992 (973/259)       14476     \n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "confidence_metrics = Metrics()\n",
    "confidence_metrics.load_metrics(\"baselines/confidence_score.pkl\")\n",
    "confidence_metrics.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c13641a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rectified TP: 5372\n",
      "Rectified FP: 4940\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rectified TP: {confidence_metrics._calculate_tp()}\")\n",
    "print(f\"Rectified FP: {confidence_metrics._calculate_fp()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09f44266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12217/12217 [00:04<00:00, 2446.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average prefix tokens: 16.637286088007023 tokens * 230118 prefixes\n",
      "Average invocation tokens: 14.684789248625535 tokens * 14733 invocations\n",
      "Total FLOPs for confidence model: 325,724,260,216,320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flops_calculator import FlopsCalculator\n",
    "flops_calculator = FlopsCalculator(\"baselines/flops_coefficients.pkl\")\n",
    "\n",
    "num_prefixes = len(y_hat)\n",
    "num_invocations = np.sum(y_hat)\n",
    "token_lengths = np.array([], dtype=int)\n",
    "invocation_token_length = np.array([], dtype=int)\n",
    "ix = 0\n",
    "for true_bio, sentence_prefixes in tqdm(prefixes):\n",
    "    invocations = []\n",
    "    for prefix in sentence_prefixes:\n",
    "        token_lengths = np.append(token_lengths, len(prefix) + 2)\n",
    "        if y_hat[ix] == 1:\n",
    "            invocation_token_length = np.append(invocation_token_length, len(prefix) + 2)  # +2 for [CLS] and [SEP]\n",
    "        ix += 1\n",
    "\n",
    "avg_prefix_tokens = np.mean(token_lengths) if token_lengths.size > 0 else 0\n",
    "avg_invocation_tokens = np.mean(invocation_token_length) if invocation_token_length.size > 0 else 0\n",
    "\n",
    "print(f\"Average prefix tokens: {avg_prefix_tokens} tokens * {num_prefixes} prefixes\")\n",
    "print(f\"Average invocation tokens: {avg_invocation_tokens} tokens * {num_invocations} invocations\")\n",
    "\n",
    "confidence_model_flops = flops_calculator.calculate_flops(\"model_1\", avg_prefix_tokens) * num_prefixes\n",
    "ner_running_flops = flops_calculator.calculate_flops(\"ner\", avg_invocation_tokens) * num_invocations\n",
    "\n",
    "total_flops = confidence_model_flops + ner_running_flops\n",
    "print(f\"Total FLOPs for confidence model: {confidence_model_flops:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd4465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
