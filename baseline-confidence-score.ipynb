{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d9b6ce",
   "metadata": {},
   "source": [
    "# Confidence Score Baseline\n",
    "\n",
    "This notebook implements the confidence score (approach 1) baseline for near real-time Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fbfba9",
   "metadata": {},
   "source": [
    "## 1. Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f81fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e084192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "350ceece",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src\")\n",
    "from utils import convert_ids_to_bio\n",
    "from confidence_model import confidence_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fe5f8",
   "metadata": {},
   "source": [
    "## 2. Load OntoNotes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ff89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with splits: dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "# Load the English portion of OntoNotes 5.0\n",
    "ontonotes = load_dataset(\n",
    "    \"conll2012_ontonotesv5\",\n",
    "    \"english_v12\",\n",
    "    cache_dir=\"./dataset/ontonotes\",\n",
    ")\n",
    "print(f\"Dataset loaded with splits: {ontonotes.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459811a1",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "Here we create all window prefixes and pre-compute the CLS token over the OntoNotes test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d919aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 1200/1200 [00:00<00:00, 1336.49doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prefixes created: 230118\n",
      "Example prefix: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['--']\n",
      "['--', 'basically']\n",
      "['--', 'basically', ',']\n",
      "['--', 'basically', ',', 'it']\n",
      "['--', 'basically', ',', 'it', 'was']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties']\n",
      "['--', 'basically', ',', 'it', 'was', 'unanimously', 'agreed', 'upon', 'by', 'the', 'various', 'relevant', 'parties', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prefixes = []\n",
    "prefix_count = 0\n",
    "\n",
    "# Iterate through the test split\n",
    "for doc in tqdm(ontonotes[\"test\"], desc=\"Processing documents\", unit=\"doc\"):\n",
    "    # Fix: Sometimes doc['sentences'] is a list of lists, so we need to flatten it\n",
    "    if isinstance(doc['sentences'], list) and isinstance(doc['sentences'][0], list):\n",
    "        doc['sentences'] = [sentence for sublist in doc['sentences'] for sentence in sublist]\n",
    "    for sentence in doc['sentences']:\n",
    "        sentence_prefixes = []\n",
    "        curr_prefix = []\n",
    "        for word in sentence['words']:\n",
    "            curr_prefix.append(word)\n",
    "            prefix_count += 1\n",
    "            sentence_prefixes.append(curr_prefix.copy())\n",
    "\n",
    "        true_bio = convert_ids_to_bio(sentence['named_entities'])\n",
    "\n",
    "        # Store the current prefix and BIO tags\n",
    "        prefixes.append((true_bio, sentence_prefixes))\n",
    "\n",
    "print(f\"Total prefixes created: {prefix_count}\")\n",
    "print(f\"Example prefix: {prefixes[0][0]}\\n{\"\\n\".join([str(x) for x in prefixes[0][1]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96340bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model = AutoModel.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "472645ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prefixes to process: 230118\n"
     ]
    }
   ],
   "source": [
    "all_prefixes = []\n",
    "for _, sentence_prefixes in prefixes:\n",
    "    all_prefixes.extend(sentence_prefixes)\n",
    "\n",
    "print(f\"Total prefixes to process: {len(all_prefixes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ddbe56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing CLS token for windows: 100%|██████████| 1798/1798 [07:45<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (230118, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate all embeddings for the prefixes in batches\n",
    "\n",
    "batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(all_prefixes), batch_size), desc=\"Computing CLS token for prefixes\"):\n",
    "    batch = all_prefixes[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "    embeddings.append(cls_token.cpu().numpy())\n",
    "\n",
    "# Unbatch the embeddings\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bbfdcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"data/ontonotes_embeddings_test_all_prefixes.npz\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c18b3954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (230118, 768)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"data/ontonotes_embeddings_test_all_prefixes.npz\")\n",
    "embeddings = data[\"embeddings\"]\n",
    "print(f\"Shape of embeddings: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9bf9b",
   "metadata": {},
   "source": [
    "## 4. Implementing confidence score baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d41621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence_model(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_model = confidence_model()\n",
    "conf_model.load_state_dict(torch.load(\"models/confidence_model.pth\", weights_only=True))\n",
    "conf_model.to(device)\n",
    "conf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c96020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
