{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fd52eb",
   "metadata": {},
   "source": [
    "# Window size model (approach 2) Baseline\n",
    "\n",
    "This notebook implements the sliding window baseline for near real-time Named Entity Recognition (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba6621",
   "metadata": {},
   "source": [
    "## 1. Setup and preparation\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10251243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8765cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06be2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"src\")\n",
    "from utils import convert_ids_to_bio, convert_predictions\n",
    "from window_slide_model import WindowSlideModel\n",
    "from metrics import Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e88fc4",
   "metadata": {},
   "source": [
    "## 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50050e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with splits: dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "# Load the English portion of OntoNotes 5.0\n",
    "ontonotes = load_dataset(\n",
    "    \"conll2012_ontonotesv5\",\n",
    "    \"english_v12\",\n",
    "    cache_dir=\"./dataset/ontonotes\",\n",
    ")\n",
    "print(f\"Dataset loaded with splits: {ontonotes.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3999e2c",
   "metadata": {},
   "source": [
    "## 3. Creating all window sizes of size 6 accross test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02e7d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows created: 224128\n",
      "Example window: ['equipment', 'and', 'the', 'engineers', 'and', 'they'], BIO: ['O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "windows = []\n",
    "bio_windows = []\n",
    "SPAN_LENGTH = 6\n",
    "\n",
    "# Iterate through the test split\n",
    "for doc in ontonotes[\"test\"]:\n",
    "    curr_window = []\n",
    "    curr_bio_window = []\n",
    "    # Fix: Sometimes doc['sentences'] is a list of lists, so we need to flatten it\n",
    "    if isinstance(doc['sentences'], list) and isinstance(doc['sentences'][0], list):\n",
    "        doc['sentences'] = [sentence for sublist in doc['sentences'] for sentence in sublist]\n",
    "    for sentence in doc[\"sentences\"]:\n",
    "        for idx, word in enumerate(sentence['words']):\n",
    "            curr_window.append(word)\n",
    "            curr_bio_window.append(sentence['named_entities'][idx])\n",
    "            # If the current window reaches the defined span length, add it to the list\n",
    "            if len(curr_window) == SPAN_LENGTH:\n",
    "                windows.append(curr_window.copy())\n",
    "                bio_windows.append(convert_ids_to_bio(curr_bio_window))\n",
    "                # Slide the window by one position\n",
    "                curr_window = curr_window[1:]\n",
    "                curr_bio_window = curr_bio_window[1:]\n",
    "\n",
    "print(f\"Total windows created: {len(windows)}\")\n",
    "ix = random.randint(0, len(windows) - 1)\n",
    "print(f\"Example window: {windows[ix]}, BIO: {bio_windows[ix]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f4a1d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model = AutoModel.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c569a2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing CLS token for windows: 100%|██████████| 1751/1751 [01:56<00:00, 15.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate all embeddings for the windows in a batch-wise manner\n",
    "batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(windows), batch_size), desc=\"Computing CLS token for windows\"):\n",
    "    batch = windows[i:i + batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "    embeddings.append(cls_token.cpu().numpy())\n",
    "\n",
    "embeddings = np.concatenate(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb145508",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"data/ner_trigger_dataset_test_embeddings.npz\", windows=windows, bio_windows=bio_windows, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d2f1f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (224128, 6); (224128, 6); (224128, 768)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"data/ner_trigger_dataset_test_embeddings.npz\")\n",
    "windows, bio_windows, embeddings = data['windows'], data['bio_windows'], data['embeddings']\n",
    "print(f\"Shape of data: {windows.shape}; {bio_windows.shape}; {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b240cf8",
   "metadata": {},
   "source": [
    "## 4. Implementing sliding window baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0df97f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowSlideModel(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_model = WindowSlideModel(embeddings.shape[1])\n",
    "window_model.load_state_dict(torch.load(\"models/bert_cls_pooling_model.pkl\", weights_only=True))\n",
    "window_model.to(device)\n",
    "window_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e960247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting BIO tags: 100%|██████████| 1751/1751 [00:00<00:00, 1993.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictions: (224128,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Determining invocation times\n",
    "batch_size = 128  # Adjust batch size based on your GPU memory\n",
    "y_hat = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(embeddings), batch_size), desc=\"Predicting BIO tags\"):\n",
    "        cls_token = torch.tensor(embeddings[i:i + batch_size]).to(device)\n",
    "        logits = window_model(cls_token)\n",
    "        y_hat_batch = (torch.sigmoid(logits) > 0.5).int()  # Convert logits to binary predictions\n",
    "\n",
    "        y_hat.append(y_hat_batch.cpu().numpy())\n",
    "\n",
    "# unbatch of y_hat\n",
    "y_hat = np.concatenate(y_hat, axis=0)\n",
    "print(f\"Shape of predictions: {y_hat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d98aa273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of invocations: 52502\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of invocations: {np.sum(y_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e5676",
   "metadata": {},
   "source": [
    "## 5. Evaludating sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11bb2c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sliding window model...\n",
      "Evaluating on 224128 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating windows: 100%|██████████| 224128/224128 [06:58<00:00, 535.36it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed!\n",
      "Metrics:\n",
      "Total NER invocations: 52502\n",
      "Avg TTFD: N/A\n",
      "Entity Type          TP         TN         FP (#B-/I-MISC)      FN        \n",
      "----------------------------------------------------------------------\n",
      "O                    N/A        221345     0                    N/A       \n",
      "B-PERSON             0          N/A        0                    9694      \n",
      "I-PERSON             0          N/A        0                    6910      \n",
      "B-NORP               0          N/A        0                    5085      \n",
      "I-NORP               0          N/A        0                    832       \n",
      "B-FAC                0          N/A        0                    546       \n",
      "I-FAC                0          N/A        0                    793       \n",
      "B-ORG                0          N/A        0                    8190      \n",
      "I-ORG                0          N/A        0                    9140      \n",
      "B-GPE                0          N/A        0                    12962     \n",
      "I-GPE                0          N/A        0                    3438      \n",
      "B-LOC                0          N/A        0                    810       \n",
      "I-LOC                0          N/A        0                    789       \n",
      "B-PRODUCT            0          N/A        0                    301       \n",
      "I-PRODUCT            0          N/A        0                    290       \n",
      "B-DATE               0          N/A        0                    6750      \n",
      "I-DATE               0          N/A        0                    7841      \n",
      "B-TIME               0          N/A        0                    665       \n",
      "I-TIME               0          N/A        0                    826       \n",
      "B-PERCENT            0          N/A        0                    1631      \n",
      "I-PERCENT            0          N/A        0                    2584      \n",
      "B-MONEY              0          N/A        0                    1240      \n",
      "I-MONEY              0          N/A        0                    2928      \n",
      "B-QUANTITY           0          N/A        0                    519       \n",
      "I-QUANTITY           0          N/A        0                    893       \n",
      "B-ORDINAL            0          N/A        0                    597       \n",
      "I-ORDINAL            0          N/A        0                    29        \n",
      "B-CARDINAL           0          N/A        0                    3605      \n",
      "I-CARDINAL           0          N/A        0                    1368      \n",
      "B-EVENT              0          N/A        0                    249       \n",
      "I-EVENT              0          N/A        0                    451       \n",
      "B-WORK_OF_ART        0          N/A        0                    446       \n",
      "I-WORK_OF_ART        0          N/A        0                    753       \n",
      "B-LAW                0          N/A        0                    123       \n",
      "I-LAW                0          N/A        0                    297       \n",
      "B-LANGUAGE           0          N/A        0                    92        \n",
      "I-LANGUAGE           0          N/A        0                    0         \n",
      "Total                0          221345     0 (0/0)              93667     \n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sliding_window_metrics = Metrics()\n",
    "\n",
    "# For sliding window evaluation, we need to simulate the streaming process\n",
    "# We'll reconstruct the document token by token and evaluate at each invocation point\n",
    "\n",
    "# First, let's load a pre-trained NER model to get actual BIO predictions when invoked\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load NER model for actual predictions\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_model.eval()\n",
    "ner_model.to(device)\n",
    "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Function to get NER predictions for a window\n",
    "def get_ner_predictions(tokens_window):\n",
    "    \"\"\"Get NER predictions for a window of tokens.\"\"\"\n",
    "    outputs = ner_pipeline(tokens_window)\n",
    "    bio_tags = convert_predictions(tokens_window, outputs)\n",
    "    return bio_tags\n",
    "\n",
    "# Group windows by document to reconstruct the streaming process\n",
    "# We need to simulate how the sliding window would work in practice\n",
    "print(\"Evaluating sliding window model...\")\n",
    "\n",
    "# Since we have sliding windows, we need to reconstruct the original document structure\n",
    "# and simulate the streaming evaluation process\n",
    "\n",
    "# For simplicity, let's evaluate on a subset of windows to demonstrate the approach\n",
    "num_windows_to_evaluate = len(windows)\n",
    "print(f\"Evaluating on {num_windows_to_evaluate} windows...\")\n",
    "\n",
    "for i in tqdm(range(num_windows_to_evaluate), desc=\"Evaluating windows\"):\n",
    "    # Get the current window and its prediction\n",
    "    current_window = windows[i]\n",
    "    current_bio_ground_truth = bio_windows[i]\n",
    "    window_prediction = y_hat[i]  # 1 if model says invoke NER, 0 otherwise\n",
    "    \n",
    "    # Simulate the streaming process:\n",
    "    # - We have tokens coming in one by one\n",
    "    # - At each step, we decide whether to invoke NER based on the sliding window prediction\n",
    "    # - When we invoke NER, we get predictions for the current window\n",
    "    \n",
    "    predictions_for_evaluation = []\n",
    "    \n",
    "    # If the sliding window model says to invoke NER (prediction = 1)\n",
    "    if window_prediction == 1:\n",
    "        # Get NER predictions for this window\n",
    "        ner_predictions = get_ner_predictions(\" \".join(current_window))\n",
    "        predictions_for_evaluation.append(ner_predictions)\n",
    "\n",
    "    # If we have predictions to evaluate\n",
    "    if predictions_for_evaluation:\n",
    "        # Evaluate metrics for this window\n",
    "        ground_truth = (current_window, current_bio_ground_truth)\n",
    "        sliding_window_metrics.evaluate_metrics(ground_truth, predictions_for_evaluation)\n",
    "\n",
    "print(\"Evaluation completed!\")\n",
    "sliding_window_metrics.print_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24527596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
