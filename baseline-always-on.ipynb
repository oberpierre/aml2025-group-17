{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e6e013",
   "metadata": {},
   "source": [
    "# Always-On NER Baseline\n",
    "\n",
    "This notebook implements the \"always-on\" baseline for near real-time Named Entity Recognition (NER). In this approach, we run NER on every token as it arrives, simulating a scenario where we have no selective inference strategy.\n",
    "\n",
    "This is the most computationally expensive approach, but it gives us the earliest possible detection of entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff271aac",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a38814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78daf5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierre/projects/uzh_repos/aml2025-group-17/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f31c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./src/\")\n",
    "from utils import convert_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffa1c2",
   "metadata": {},
   "source": [
    "## 2. Load OntoNotes Dataset\n",
    "\n",
    "We'll use the OntoNotes 5.0 dataset, which contains texts from various genres with named entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb372455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 194M/194M [00:04<00:00, 42.4MB/s] \n",
      "Generating train split: 100%|██████████| 10539/10539 [00:10<00:00, 958.80 examples/s] \n",
      "Generating validation split: 100%|██████████| 1370/1370 [00:01<00:00, 862.76 examples/s]\n",
      "Generating test split: 100%|██████████| 1200/1200 [00:01<00:00, 1133.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with splits: dict_keys(['train', 'validation', 'test'])\n",
      "Train set: 10539 examples\n",
      "Validation set: 1370 examples\n",
      "Test set: 1200 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the English portion of OntoNotes 5.0\n",
    "ontonotes = load_dataset(\n",
    "    \"conll2012_ontonotesv5\",\n",
    "    \"english_v12\",\n",
    "    cache_dir=\"./dataset/ontonotes\",\n",
    ")\n",
    "print(f\"Dataset loaded with splits: {ontonotes.keys()}\")\n",
    "\n",
    "# Get basic statistics for each split\n",
    "for split_name in ontonotes.keys():\n",
    "    print(f\"{split_name.capitalize()} set: {len(ontonotes[split_name])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60293d19",
   "metadata": {},
   "source": [
    "## 3. Set Up NER Model\n",
    "\n",
    "We'll use a pre-trained transformer model for NER. For this baseline, we'll use a BERT-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c008c811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: John Smith works at Microsoft in Seattle.\n",
      "NER pipeline output: [{'entity_group': 'PER', 'score': np.float32(0.9996886), 'word': 'John Smith', 'start': 0, 'end': 10}, {'entity_group': 'ORG', 'score': np.float32(0.9989378), 'word': 'Microsoft', 'start': 20, 'end': 29}, {'entity_group': 'LOC', 'score': np.float32(0.9988439), 'word': 'Seattle', 'start': 33, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained NER model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Wrapper function for NER that takes tokens and returns predictions\n",
    "def run_ner_on_tokens(tokens):\n",
    "    \"\"\"Run NER on a list of tokens.\"\"\"\n",
    "    text = \" \".join(tokens)\n",
    "    pipeline_output = ner_pipeline(text)\n",
    "    return pipeline_output\n",
    "\n",
    "# Test NER model on a sample sentence\n",
    "sample_text = \"John Smith works at Microsoft in Seattle.\"\n",
    "print(f\"Sample text: {sample_text}\")\n",
    "test_output = ner_pipeline(sample_text)\n",
    "print(f\"NER pipeline output: {test_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2896f89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['John', 'Smith', 'works', 'at', 'Microsoft', 'in', 'Seattle']\n",
      "Converted BIO tags: ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O', 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Removing punctuation to prevent mismatches and splitting the sample text into tokens\n",
    "tokens = re.sub(r\"[.,?!]+\", \"\", sample_text).split(\" \")\n",
    "\n",
    "bio_tags = convert_predictions(tokens, test_output)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Converted BIO tags: {bio_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a013e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
