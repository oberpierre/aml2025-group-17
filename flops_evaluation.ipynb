{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6439d520",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e01c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ner_text = \"\"\"\n",
    "The Old Zurich War was a conflict between the canton of Zurich and the other seven cantons of the Old Swiss Confederacy over the succession to the Count of Toggenburg.\n",
    "In 1436, Count Friedrich VII of Toggenburg died, leaving neither heir nor will.\n",
    "The canton of Zurich, led by burgomaster Rudolf StÃ¼ssi, claimed the Toggenburg lands; the cantons of Schwyz and Glarus made counter-claims, backed by the other cantons.\n",
    "In 1438, Zurich occupied the disputed area and cut off grain supplies to Schwyz and Glarus. In 1440, the other cantons expelled Zurich from the confederation and declared war.\n",
    "Zurich retaliated by making an alliance with Frederick III, Holy Roman Emperor of the house of Habsburg.\n",
    "\n",
    "The forces of Zurich were defeated in the Battle of St. Jakob an der Sihl on 22 July 1443 and Zurich was besieged.\n",
    "Frederick appealed to Charles VII of France to attack the confederates and the latter sent a force of about 30,000 Armagnac mercenaries under the command of the Dauphin via Basel to relieve the city.\n",
    "In the Battle of St. Jakob an der Birs near Basel on 26 August 1444, a blocking force of roughly 1,600 Swiss confederates was defeated, but inflicted such heavy losses on the French (2,000 killed) that the Dauphin decided to retreat.\n",
    "The confederacy and the Dauphin concluded a peace in October 1444, and his mercenary army withdrew from the war altogether.\n",
    "\n",
    "In May 1444, the confederacy laid siege to Greifensee, and captured the town after four weeks, on May 27, beheading all but two of the 64 defenders the next day, including their leader, Wildhans von Breitenlandenberg, the so-called Murder of Greifensee.\n",
    "Even in this time of war, such a mass execution was widely considered a cruel and unjust deed.\n",
    "\"\"\"\n",
    "\n",
    "small_ner_text = \"John Doe was here.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125c0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fvcore in ./.venv/lib/python3.13/site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from fvcore) (2.3.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in ./.venv/lib/python3.13/site-packages (from fvcore) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from fvcore) (6.0.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from fvcore) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in ./.venv/lib/python3.13/site-packages (from fvcore) (3.1.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.13/site-packages (from fvcore) (11.2.1)\n",
      "Requirement already satisfied: tabulate in ./.venv/lib/python3.13/site-packages (from fvcore) (0.9.0)\n",
      "Requirement already satisfied: iopath>=0.1.7 in ./.venv/lib/python3.13/site-packages (from fvcore) (0.1.10)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.13/site-packages (from iopath>=0.1.7->fvcore) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fvcore torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab34cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "\n",
    "def bert_input(token_embeddings):\n",
    "    \"\"\"\n",
    "    Helper function to prepare BERT input embeddings for fvcore.\n",
    "    \n",
    "    Args:\n",
    "        token_embeddings: Dictionary containing 'input_ids', 'attention_mask', and optionally 'token_type_ids'.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of tensors that fvcore can handle.\n",
    "    \"\"\"\n",
    "    # Convert BatchEncoding to tuple of tensors\n",
    "    input_ids = token_embeddings[\"input_ids\"]\n",
    "    attention_mask = token_embeddings[\"attention_mask\"]\n",
    "    \n",
    "    # Check if token_type_ids exists, if not create zeros\n",
    "    if \"token_type_ids\" in token_embeddings and token_embeddings[\"token_type_ids\"] is not None:\n",
    "        token_type_ids = token_embeddings[\"token_type_ids\"]\n",
    "    else:\n",
    "        token_type_ids = torch.zeros_like(input_ids)\n",
    "    \n",
    "    return (input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "def measure_flops(model, input):\n",
    "    \"\"\"\n",
    "    Helper function to measure FLOPS for any given function execution.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        input: The input for the model (tuple of tensors)\n",
    "\n",
    "    Returns:\n",
    "        FlopCountAnalysis: An object containing the FLOPS count and other details\n",
    "    \"\"\"\n",
    "    # Use fvcore to count FLOPS\n",
    "    with torch.no_grad():\n",
    "        flops = FlopCountAnalysis(model, input)\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31573df3",
   "metadata": {},
   "source": [
    "## 1.1. Evaluating average token lengths for ontonotes test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df77af5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0bd4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierre/projects/uzh_repos/aml2025-group-17/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the English portion of OntoNotes 5.0\n",
    "ontonotes = load_dataset(\n",
    "    \"conll2012_ontonotesv5\",\n",
    "    \"english_v12\",\n",
    "    cache_dir=\"./dataset/ontonotes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ad2cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prefixes: 230,118; Average token length: 14.64\n"
     ]
    }
   ],
   "source": [
    "# Evaluating number of prefixes and average token length per prefix\n",
    "def evaluate_prefixes(dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the number of prefixes and average token length per prefix in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: List of documents (dictionary) containing 'sentences' (list of dictionaries) containing 'words' (list of strings).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (number of prefixes, average token length)\n",
    "    \"\"\"\n",
    "    num_prefixes = 0\n",
    "    total_prefix_token_length = 0\n",
    "    for doc in dataset:\n",
    "        for sent in doc['sentences']:\n",
    "            num_tokens = len(sent['words'])\n",
    "            num_prefixes += num_tokens\n",
    "            total_prefix_token_length += num_tokens * (num_tokens + 1) // 2\n",
    "    avg_prefix_length = total_prefix_token_length / num_prefixes if num_prefixes > 0 else 0\n",
    "    return num_prefixes, avg_prefix_length\n",
    "\n",
    "# Evaluate prefixes in the OntoNotes dataset\n",
    "num_prefixes, avg_prefix_length = evaluate_prefixes(ontonotes['test'])\n",
    "print(f\"Number of prefixes: {num_prefixes:,}; Average token length: {avg_prefix_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbaf9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows: 224,128; Average window length: 6.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate number of windows and average window length\n",
    "# See precompute_fixed_spans_and_labels.ipynb for the code that counts the windows\n",
    "num_windows, avg_window_length = 224128, 6\n",
    "print(f\"Number of windows: {num_windows:,}; Average window length: {avg_window_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba7f7a",
   "metadata": {},
   "source": [
    "## 2. Evaluating FLOPS of the baseline BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c93100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60145bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', 'input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'labels', 'output_attentions', 'output_hidden_states', 'return_dict', 'outputs', 'sequence_output', 'logits', 'loss', 'loss_fct', 'output')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained NER model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(model.forward.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05466e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text tokens: torch.Size([1, 418])\n",
      "Small text tokens: torch.Size([1, 8])\n",
      "\n",
      "FLOPS Results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text FLOPS: 35,545,703,424\n",
      "Long text FLOPS/token: 85,037,568.00\n",
      "============================================================\n",
      "Small text FLOPS: 680,300,544\n",
      "Small text FLOPS/token: 85,037,568.00\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize both texts\n",
    "long_tokens = tokenizer(long_ner_text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "small_tokens = tokenizer(small_ner_text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "print(f\"Long text tokens: {long_tokens['input_ids'].shape}\")\n",
    "print(f\"Small text tokens: {small_tokens['input_ids'].shape}\")\n",
    "\n",
    "# Evaluate FLOPS for long text given it's input_ids and attention_mask\n",
    "long_flops = measure_flops(model, bert_input(long_tokens))\n",
    "\n",
    "# Evaluate FLOPS for small text\n",
    "small_flops = measure_flops(model, bert_input(small_tokens))\n",
    "\n",
    "print(f\"\\nFLOPS Results:\")\n",
    "print(f\"Long text FLOPS: {long_flops.total():,}\")\n",
    "print(f\"Long text FLOPS/token: {long_flops.total() / long_tokens['input_ids'].numel():,.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Small text FLOPS: {small_flops.total():,}\")\n",
    "print(f\"Small text FLOPS/token: {small_flops.total() / small_tokens['input_ids'].numel():,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54803a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of embeddings in X: (72860, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/ontonotes_embeddings_test.npz')\n",
    "bert_X = data['X']\n",
    "\n",
    "# Get average length of embeddings in X\n",
    "# average_length = np.mean([x.shape[0] for x in X])\n",
    "print(f\"Average length of embeddings in X: {bert_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f63fb6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring BERT FLOPS for different sequence lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=4: 340,150,272 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=8: 680,300,544 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=12: 1,020,450,816 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=16: 1,360,601,088 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=24: 2,040,901,632 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=32: 2,721,202,176 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=48: 4,081,803,264 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=64: 5,442,404,352 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=96: 8,163,606,528 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=128: 10,884,808,704 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=192: 16,327,213,056 FLOPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=256: 21,769,617,408 FLOPS\n",
      "\n",
      "BERT FLOPS = -0.03 * L^2 + 85037576.00 * L + 16.60\n",
      "R-squared: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a more comprehensive regression model for the flops\n",
    "bert_lengths_to_sample = [4, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256]\n",
    "bert_tokens = tokenizer(long_ner_text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "bert_flops_per_length = {}\n",
    "\n",
    "print(\"Measuring BERT FLOPS for different sequence lengths...\")\n",
    "for L in bert_lengths_to_sample:\n",
    "    # Create tokens of exact length L\n",
    "    tokens = {k: v[:, :L] if v.shape[1] > L else v for k, v in bert_tokens.items()}\n",
    "    \n",
    "    # Pad if necessary\n",
    "    if tokens[\"input_ids\"].shape[1] < L:\n",
    "        pad_length = L - tokens[\"input_ids\"].shape[1]\n",
    "        tokens[\"input_ids\"] = torch.cat([tokens[\"input_ids\"], torch.zeros(1, pad_length, dtype=torch.long)], dim=1)\n",
    "        tokens[\"attention_mask\"] = torch.cat([tokens[\"attention_mask\"], torch.zeros(1, pad_length, dtype=torch.long)], dim=1)\n",
    "        if \"token_type_ids\" in tokens:\n",
    "            tokens[\"token_type_ids\"] = torch.cat([tokens[\"token_type_ids\"], torch.zeros(1, pad_length, dtype=torch.long)], dim=1)\n",
    "    \n",
    "    assert tokens[\"input_ids\"].shape[1] == L, f\"Expected {L} tokens, got {tokens['input_ids'].shape[1]}\"\n",
    "    \n",
    "    flops = FlopCountAnalysis(model, bert_input(tokens)).total()\n",
    "    bert_flops_per_length[L] = flops\n",
    "    print(f\"L={L}: {flops:,} FLOPS\")\n",
    "\n",
    "# Convert to arrays\n",
    "bert_X = torch.tensor(list(bert_flops_per_length.keys()), dtype=torch.float32).unsqueeze(1)\n",
    "bert_y = torch.tensor(list(bert_flops_per_length.values()), dtype=torch.float32)\n",
    "\n",
    "# Fit more comprehensive polynomial: FLOPs = a * L^2 + b * L + c\n",
    "bert_X_poly = torch.cat([bert_X**2, bert_X, torch.ones_like(bert_X)], dim=1)\n",
    "bert_coeffs = torch.linalg.lstsq(bert_X_poly, bert_y).solution  # returns [a, b, c]\n",
    "print(f\"\\nBERT FLOPS = {bert_coeffs[0].item():.2f} * L^2 + {bert_coeffs[1].item():.2f} * L + {bert_coeffs[2].item():.2f}\")\n",
    "\n",
    "# Calculate R-squared for validation\n",
    "bert_y_pred = bert_X_poly @ bert_coeffs\n",
    "bert_ss_res = torch.sum((bert_y - bert_y_pred) ** 2)\n",
    "bert_ss_tot = torch.sum((bert_y - torch.mean(bert_y)) ** 2)\n",
    "bert_r_squared = 1 - bert_ss_res / bert_ss_tot\n",
    "print(f\"R-squared: {bert_r_squared.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be1b41",
   "metadata": {},
   "source": [
    "## 3. Evaluating FLOPS for our prefix evaluating MLP (approach 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e6e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src/\")\n",
    "from confidence_model import confidence_model\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f8d81ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence_model(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model.eval()\n",
    "confidence_model = confidence_model()\n",
    "confidence_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babcc534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text tokens: torch.Size([1, 768])\n",
      "Small text tokens: torch.Size([1, 768])\n",
      "\n",
      "FLOPS Results:\n",
      "Long text FLOPS CLS: 35,543,404,032\n",
      "Long text FLOPS Model: 196,864\n",
      "Long text FLOPS Total: 35,543,600,896\n",
      "Long text FLOPS/token: 46,280,730.33\n",
      "============================================================\n",
      "Small text FLOPS CLS: 680,835,072\n",
      "Small text FLOPS Model: 196,864\n",
      "Small text FLOPS Total: 681,031,936\n",
      "Small text FLOPS/token: 886,760.33\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate FLOPS for long text given it's input_ids and attention_mask\n",
    "moodel_1_long_flops_prepare = measure_flops(bert_model, bert_input(long_tokens))\n",
    "moodel_1_long_tokens_cls = bert_model(**long_tokens).last_hidden_state[:, 0, :]\n",
    "moodel_1_long_flops_run = measure_flops(confidence_model, moodel_1_long_tokens_cls)\n",
    "moodel_1_long_flops = moodel_1_long_flops_prepare.total() + moodel_1_long_flops_run.total()\n",
    "\n",
    "# Evaluate FLOPS for small text\n",
    "moodel_1_small_flops_prepare = measure_flops(bert_model, bert_input(small_tokens))\n",
    "moodel_1_small_tokens_cls = bert_model(**small_tokens).last_hidden_state[:, 0, :]\n",
    "moodel_1_small_flops_run = measure_flops(confidence_model, moodel_1_small_tokens_cls)\n",
    "moodel_1_small_flops = moodel_1_small_flops_prepare.total() + moodel_1_small_flops_run.total()\n",
    "\n",
    "print(f\"Long text tokens: {moodel_1_long_tokens_cls.shape}\")\n",
    "print(f\"Small text tokens: {moodel_1_small_tokens_cls.shape}\")\n",
    "\n",
    "print(f\"\\nFLOPS Results:\")\n",
    "print(f\"Long text FLOPS CLS: {moodel_1_long_flops_prepare.total():,}\")\n",
    "print(f\"Long text FLOPS Model: {moodel_1_long_flops_run.total():,}\")\n",
    "print(f\"Long text FLOPS Total: {moodel_1_long_flops:,}\")\n",
    "print(f\"Long text FLOPS/token: {moodel_1_long_flops / moodel_1_long_tokens_cls.numel():,.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Small text FLOPS CLS: {moodel_1_small_flops_prepare.total():,}\")\n",
    "print(f\"Small text FLOPS Model: {moodel_1_small_flops_run.total():,}\")\n",
    "print(f\"Small text FLOPS Total: {moodel_1_small_flops:,}\")\n",
    "print(f\"Small text FLOPS/token: {moodel_1_small_flops / moodel_1_small_tokens_cls.numel():,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c44b1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring Model 1 FLOPS for different sequence lengths...\n",
      "L=4: 340,909,312 FLOPS (BERT: 340,712,448, MLP: 196,864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=8: 681,031,936 FLOPS (BERT: 680,835,072, MLP: 196,864)\n",
      "L=12: 1,021,154,560 FLOPS (BERT: 1,020,957,696, MLP: 196,864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=16: 1,361,277,184 FLOPS (BERT: 1,361,080,320, MLP: 196,864)\n",
      "L=24: 2,041,522,432 FLOPS (BERT: 2,041,325,568, MLP: 196,864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=32: 2,721,767,680 FLOPS (BERT: 2,721,570,816, MLP: 196,864)\n",
      "L=48: 4,082,258,176 FLOPS (BERT: 4,082,061,312, MLP: 196,864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=64: 5,442,748,672 FLOPS (BERT: 5,442,551,808, MLP: 196,864)\n",
      "L=96: 8,163,729,664 FLOPS (BERT: 8,163,532,800, MLP: 196,864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=128: 10,884,710,656 FLOPS (BERT: 10,884,513,792, MLP: 196,864)\n",
      "L=192: 16,326,672,640 FLOPS (BERT: 16,326,475,776, MLP: 196,864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=256: 21,768,634,624 FLOPS (BERT: 21,768,437,760, MLP: 196,864)\n",
      "\n",
      "Model 1 FLOPS = 0.03 * L^2 + 85030656.00 * L + 786886.00\n",
      "R-squared: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a more comprehensive regression model for Model 1\n",
    "model_1_lengths_to_sample = [4, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 256]\n",
    "model_1_tokens = tokenizer(long_ner_text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "model_1_flops_per_length = {}\n",
    "\n",
    "print(\"Measuring Model 1 FLOPS for different sequence lengths...\")\n",
    "for L in model_1_lengths_to_sample:\n",
    "    # Create tokens of exact length L\n",
    "    tokens = {k: v[:, :L] if v.shape[1] > L else v for k, v in model_1_tokens.items()}\n",
    "    \n",
    "    # Pad if necessary\n",
    "    if tokens[\"input_ids\"].shape[1] < L:\n",
    "        pad_length = L - tokens[\"input_ids\"].shape[1]\n",
    "        tokens[\"input_ids\"] = torch.cat([tokens[\"input_ids\"], torch.zeros(1, pad_length, dtype=torch.long)], dim=1)\n",
    "        tokens[\"attention_mask\"] = torch.cat([tokens[\"attention_mask\"], torch.zeros(1, pad_length, dtype=torch.long)], dim=1)\n",
    "        if \"token_type_ids\" in tokens:\n",
    "            tokens[\"token_type_ids\"] = torch.cat([tokens[\"token_type_ids\"], torch.zeros(1, pad_length, dtype=torch.long)], dim=1)\n",
    "    \n",
    "    # Measure BERT FLOPS\n",
    "    with torch.no_grad():\n",
    "        cls_token = bert_model(**tokens).last_hidden_state[:, 0, :]\n",
    "    flops_prepare = measure_flops(bert_model, bert_input(tokens)).total()\n",
    "    flops_run = measure_flops(confidence_model, cls_token).total()\n",
    "    flops = flops_prepare + flops_run\n",
    "    model_1_flops_per_length[L] = flops\n",
    "    print(f\"L={L}: {flops:,} FLOPS (BERT: {flops_prepare:,}, MLP: {flops_run:,})\")\n",
    "\n",
    "# Convert to arrays\n",
    "model_1_X = torch.tensor(list(model_1_flops_per_length.keys()), dtype=torch.float32).unsqueeze(1)\n",
    "model_1_y = torch.tensor(list(model_1_flops_per_length.values()), dtype=torch.float32)\n",
    "\n",
    "# Fit comprehensive polynomial: FLOPs = a * L^2 + b * L + c\n",
    "model_1_X_poly = torch.cat([model_1_X**2, model_1_X, torch.ones_like(model_1_X)], dim=1)\n",
    "model_1_coeffs = torch.linalg.lstsq(model_1_X_poly, model_1_y).solution  # returns [a, b, c]\n",
    "print(f\"\\nModel 1 FLOPS = {model_1_coeffs[0].item():.2f} * L^2 + {model_1_coeffs[1].item():.2f} * L + {model_1_coeffs[2].item():.2f}\")\n",
    "\n",
    "# Calculate R-squared for validation\n",
    "model_1_y_pred = model_1_X_poly @ model_1_coeffs\n",
    "model_1_ss_res = torch.sum((model_1_y - model_1_y_pred) ** 2)\n",
    "model_1_ss_tot = torch.sum((model_1_y - torch.mean(model_1_y)) ** 2)\n",
    "model_1_r_squared = 1 - model_1_ss_res / model_1_ss_tot\n",
    "print(f\"R-squared: {model_1_r_squared.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0174c",
   "metadata": {},
   "source": [
    "## 4. Evaluating FLOPS for our sliding window evaluating MLP (approach 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11ced94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_slide_model import WindowSlideModel\n",
    "window_model = WindowSlideModel(input_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d43d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text tokens: torch.Size([1, 768])\n",
      "\n",
      "FLOPS Results:\n",
      "Model 2 FLOPS CLS: 510,773,760\n",
      "Model 2 FLOPS Model: 98,432\n",
      "Model 2 FLOPS Total: 510,872,192\n",
      "Model 2 FLOPS/token: 85,145,365.33\n"
     ]
    }
   ],
   "source": [
    "model_2_text = \" \".join(long_ner_text.split()[0:4]) # Using a window size of 6 as per the problem statement\n",
    "model_2_tokens = tokenizer(model_2_text, return_tensors=\"pt\", padding=False, truncation=False)\n",
    "assert model_2_tokens[\"input_ids\"].shape[1] == 6, f\"Expected 6 tokens, got {model_2_tokens['input_ids'].shape[1]}\"\n",
    "model_2_long_flops_prepare = measure_flops(bert_model, bert_input(model_2_tokens))\n",
    "model_2_long_tokens_cls = bert_model(**model_2_tokens).last_hidden_state[:, 0, :]\n",
    "model_2_long_flops_run = measure_flops(window_model, model_2_long_tokens_cls)\n",
    "model_2_long_flops = model_2_long_flops_prepare.total() + model_2_long_flops_run.total()\n",
    "model_2_long_flops_per_token = model_2_long_flops / model_2_tokens[\"input_ids\"].numel()\n",
    "print(f\"Long text tokens: {model_2_long_tokens_cls.shape}\")\n",
    "\n",
    "print(f\"\\nFLOPS Results:\")\n",
    "print(f\"Model 2 FLOPS CLS: {model_2_long_flops_prepare.total():,}\")\n",
    "print(f\"Model 2 FLOPS Model: {model_2_long_flops_run.total():,}\")\n",
    "print(f\"Model 2 FLOPS Total: {model_2_long_flops:,}\")\n",
    "print(f\"Model 2 FLOPS/token: {model_2_long_flops_per_token:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a5263",
   "metadata": {},
   "source": [
    "## 5. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da0896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the coefficients for later use\n",
    "with open('baselines/flops_coefficients.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'bert_coeffs': bert_coeffs,\n",
    "        'model_1_coeffs': model_1_coeffs,\n",
    "        'model_2_flops_per_token': model_2_long_flops_per_token\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "003da6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.flops_calculator import FlopsCalculator\n",
    "flops_calculator = FlopsCalculator(\"baselines/flops_coefficients.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab18d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FLOPS Comparison ===\n",
      "Sequence length: 418 tokens\n",
      "Window length: 6 tokens\n",
      "\n",
      "NER - Estimated: 35,545,702,400\n",
      "NER - Actual: 35,545,703,424\n",
      "NER - Error: 0.0%\n",
      "\n",
      "Model 1 - Estimated: 35,543,605,248\n",
      "Model 1 - Actual: 35,543,600,896\n",
      "Model 1 - Error: 0.0%\n",
      "\n",
      "Model 2 - Estimated: 510,872,192\n",
      "Model 2 - Actual: 510,872,192\n",
      "Model 2 - Error: 0.0%\n"
     ]
    }
   ],
   "source": [
    "long_text_len = long_tokens[\"input_ids\"].shape[1]\n",
    "long_text_window_len = model_2_tokens[\"input_ids\"].shape[1]\n",
    "\n",
    "# Get actual measured values for comparison\n",
    "actual_ner_flops = long_flops.total()\n",
    "actual_model1_flops = moodel_1_long_flops\n",
    "actual_model2_flops = model_2_long_flops\n",
    "\n",
    "print(f\"=== FLOPS Comparison ===\")\n",
    "print(f\"Sequence length: {long_text_len} tokens\")\n",
    "print(f\"Window length: {long_text_window_len} tokens\")\n",
    "print()\n",
    "\n",
    "estimated_ner = flops_calculator.calculate_flops(\"ner\", long_text_len)\n",
    "print(f\"NER - Estimated: {estimated_ner:,}\")\n",
    "print(f\"NER - Actual: {actual_ner_flops:,}\")\n",
    "print(f\"NER - Error: {abs(estimated_ner - actual_ner_flops) / actual_ner_flops * 100:.1f}%\")\n",
    "print()\n",
    "\n",
    "estimated_model1 = flops_calculator.calculate_flops(\"model_1\", long_text_len)\n",
    "print(f\"Model 1 - Estimated: {estimated_model1:,}\")\n",
    "print(f\"Model 1 - Actual: {actual_model1_flops:,}\")\n",
    "print(f\"Model 1 - Error: {abs(estimated_model1 - actual_model1_flops) / actual_model1_flops * 100:.1f}%\")\n",
    "print()\n",
    "\n",
    "estimated_model2 = flops_calculator.calculate_flops(\"model_2\", long_text_window_len)\n",
    "print(f\"Model 2 - Estimated: {estimated_model2:,}\")\n",
    "print(f\"Model 2 - Actual: {actual_model2_flops:,}\")\n",
    "print(f\"Model 2 - Error: {abs(estimated_model2 - actual_model2_flops) / actual_model2_flops * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1c63e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FLOPS Comparison (Small Text) ===\n",
      "Sequence length: 8 tokens\n",
      "\n",
      "NER - Estimated: 680,300,608\n",
      "NER - Actual: 680,300,544\n",
      "NER - Error: 0.0%\n",
      "\n",
      "Model 1 - Estimated: 681,032,128\n",
      "Model 1 - Actual: 681,031,936\n",
      "Model 1 - Error: 0.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_text_len = small_tokens[\"input_ids\"].shape[1]\n",
    "\n",
    "# Get actual measured values for comparison\n",
    "actual_ner_flops = small_flops.total()\n",
    "actual_model1_flops = moodel_1_small_flops\n",
    "\n",
    "print(f\"=== FLOPS Comparison (Small Text) ===\")\n",
    "print(f\"Sequence length: {small_text_len} tokens\")\n",
    "print()\n",
    "\n",
    "estimated_ner = flops_calculator.calculate_flops(\"ner\", small_text_len)\n",
    "print(f\"NER - Estimated: {estimated_ner:,}\")\n",
    "print(f\"NER - Actual: {actual_ner_flops:,}\")\n",
    "print(f\"NER - Error: {abs(estimated_ner - actual_ner_flops) / actual_ner_flops * 100:.1f}%\")\n",
    "print()\n",
    "\n",
    "estimated_model1 = flops_calculator.calculate_flops(\"model_1\", small_text_len)\n",
    "print(f\"Model 1 - Estimated: {estimated_model1:,}\")\n",
    "print(f\"Model 1 - Actual: {actual_model1_flops:,}\")\n",
    "print(f\"Model 1 - Error: {abs(estimated_model1 - actual_model1_flops) / actual_model1_flops * 100:.1f}%\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d29fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
