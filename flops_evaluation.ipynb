{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6439d520",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e01c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ner_text = \"\"\"\n",
    "The Old Zurich War was a conflict between the canton of Zurich and the other seven cantons of the Old Swiss Confederacy over the succession to the Count of Toggenburg.\n",
    "In 1436, Count Friedrich VII of Toggenburg died, leaving neither heir nor will.\n",
    "The canton of Zurich, led by burgomaster Rudolf StÃ¼ssi, claimed the Toggenburg lands; the cantons of Schwyz and Glarus made counter-claims, backed by the other cantons.\n",
    "In 1438, Zurich occupied the disputed area and cut off grain supplies to Schwyz and Glarus. In 1440, the other cantons expelled Zurich from the confederation and declared war.\n",
    "Zurich retaliated by making an alliance with Frederick III, Holy Roman Emperor of the house of Habsburg.\n",
    "\"\"\"\n",
    "\n",
    "small_ner_text = \"John Doe was here.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125c0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fvcore in ./.venv/lib/python3.13/site-packages (0.1.5.post20221221)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.7.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from fvcore) (2.3.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in ./.venv/lib/python3.13/site-packages (from fvcore) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from fvcore) (6.0.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (from fvcore) (4.67.1)\n",
      "Requirement already satisfied: termcolor>=1.1 in ./.venv/lib/python3.13/site-packages (from fvcore) (3.1.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.13/site-packages (from fvcore) (11.2.1)\n",
      "Requirement already satisfied: tabulate in ./.venv/lib/python3.13/site-packages (from fvcore) (0.9.0)\n",
      "Requirement already satisfied: iopath>=0.1.7 in ./.venv/lib/python3.13/site-packages (from fvcore) (0.1.10)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.13/site-packages (from iopath>=0.1.7->fvcore) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fvcore torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab34cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
    "\n",
    "def bert_input(token_embeddings):\n",
    "    \"\"\"\n",
    "    Helper function to prepare BERT input embeddings. As per the model's forward method:\n",
    "    'self', 'input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'labels', 'output_attentions', 'output_hidden_states', 'return_dict', 'outputs', 'sequence_output', 'logits', 'loss', 'loss_fct', 'output'\n",
    "    \n",
    "    Args:\n",
    "        token_embeddings: Dictionary containing 'input_ids', 'attention_mask', and optionally 'token_type_ids'.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of tensors (input_ids, attention_mask, token_type_ids).\n",
    "    \"\"\"\n",
    "    # Convert to tuple of tensors (in order of model forward args)\n",
    "    input_ids = token_embeddings[\"input_ids\"]\n",
    "    attention_mask = token_embeddings[\"attention_mask\"]\n",
    "    token_type_ids = token_embeddings.get(\"token_type_ids\", None)\n",
    "\n",
    "    # Create a tuple of positional arguments matching model.forward signature\n",
    "    args = (input_ids, attention_mask, token_type_ids)\n",
    "    if token_type_ids is not None:\n",
    "        args += (token_type_ids,)\n",
    "    \n",
    "    return args\n",
    "\n",
    "def measure_flops(model, input):\n",
    "    \"\"\"\n",
    "    Helper function to measure FLOPS for any given function execution.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        token_embeddings: The token embeddings for the model\n",
    "\n",
    "    Returns:\n",
    "        FlopCountAnalysis: An object containing the FLOPS count and other details\n",
    "    \"\"\"\n",
    "\n",
    "    # Use fvcore to count FLOPS\n",
    "    with torch.no_grad():\n",
    "        flops = FlopCountAnalysis(model, input)\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31573df3",
   "metadata": {},
   "source": [
    "## 1.1. Evaluating average token lengths for ontonotes test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df77af5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.13/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bd4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the English portion of OntoNotes 5.0\n",
    "ontonotes = load_dataset(\n",
    "    \"conll2012_ontonotesv5\",\n",
    "    \"english_v12\",\n",
    "    cache_dir=\"./dataset/ontonotes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9ad2cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prefixes: 230,118; Average token length: 14.64\n"
     ]
    }
   ],
   "source": [
    "# Evaluating number of prefixes and average token length per prefix\n",
    "def evaluate_prefixes(dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the number of prefixes and average token length per prefix in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: List of documents (dictionary) containing 'sentences' (list of dictionaries) containing 'words' (list of strings).\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (number of prefixes, average token length)\n",
    "    \"\"\"\n",
    "    num_prefixes = 0\n",
    "    total_prefix_token_length = 0\n",
    "    for doc in dataset:\n",
    "        for sent in doc['sentences']:\n",
    "            num_tokens = len(sent['words'])\n",
    "            num_prefixes += num_tokens\n",
    "            total_prefix_token_length += num_tokens * (num_tokens + 1) // 2\n",
    "    avg_prefix_length = total_prefix_token_length / num_prefixes if num_prefixes > 0 else 0\n",
    "    return num_prefixes, avg_prefix_length\n",
    "\n",
    "# Evaluate prefixes in the OntoNotes dataset\n",
    "num_prefixes, avg_prefix_length = evaluate_prefixes(ontonotes['test'])\n",
    "print(f\"Number of prefixes: {num_prefixes:,}; Average token length: {avg_prefix_length:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbaf9d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows: 224,128; Average window length: 6.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate number of windows and average window length\n",
    "# See precompute_fixed_spans_and_labels.ipynb for the code that counts the windows\n",
    "num_windows, avg_window_length = 224128, 6\n",
    "print(f\"Number of windows: {num_windows:,}; Average window length: {avg_window_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba7f7a",
   "metadata": {},
   "source": [
    "## 2. Evaluating FLOPS of the baseline BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c93100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./.venv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60145bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', 'input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'labels', 'output_attentions', 'output_hidden_states', 'return_dict', 'outputs', 'sequence_output', 'logits', 'loss', 'loss_fct', 'output')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained NER model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(model.forward.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05466e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text tokens: torch.Size([1, 167])\n",
      "Small text tokens: torch.Size([1, 8])\n",
      "\n",
      "FLOPS Results:\n",
      "Long text FLOPS: 14,201,273,856\n",
      "Long text FLOPS/token: 85,037,568.00\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small text FLOPS: 680,300,544\n",
      "Small text FLOPS/token: 85,037,568.00\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize both texts\n",
    "long_tokens = tokenizer(long_ner_text, return_tensors=\"pt\")\n",
    "small_tokens = tokenizer(small_ner_text, return_tensors=\"pt\")\n",
    "bert_model = AutoModel.from_pretrained(\"dslim/bert-base-NER\")\n",
    "print(f\"Long text tokens: {long_tokens['input_ids'].shape}\")\n",
    "print(f\"Small text tokens: {small_tokens['input_ids'].shape}\")\n",
    "\n",
    "# Evaluate FLOPS for long text given it's input_ids and attention_mask\n",
    "long_flops = measure_flops(model, bert_input(long_tokens))\n",
    "\n",
    "# Evaluate FLOPS for small text\n",
    "small_flops = measure_flops(model, bert_input(small_tokens))\n",
    "\n",
    "print(f\"\\nFLOPS Results:\")\n",
    "print(f\"Long text FLOPS: {long_flops.total():,}\")\n",
    "print(f\"Long text FLOPS/token: {long_flops.total() / long_tokens['input_ids'].numel():,.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Small text FLOPS: {small_flops.total():,}\")\n",
    "print(f\"Small text FLOPS/token: {small_flops.total() / small_tokens['input_ids'].numel():,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54803a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of embeddings in X: (72860, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/ontonotes_embeddings_test.npz')\n",
    "X = data['X']\n",
    "\n",
    "# Get average length of embeddings in X\n",
    "# average_length = np.mean([x.shape[0] for x in X])\n",
    "print(f\"Average length of embeddings in X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63fb6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "bert.encoder.layer.0.attention.self.dropout, bert.encoder.layer.1.attention.self.dropout, bert.encoder.layer.10.attention.self.dropout, bert.encoder.layer.11.attention.self.dropout, bert.encoder.layer.2.attention.self.dropout, bert.encoder.layer.3.attention.self.dropout, bert.encoder.layer.4.attention.self.dropout, bert.encoder.layer.5.attention.self.dropout, bert.encoder.layer.6.attention.self.dropout, bert.encoder.layer.7.attention.self.dropout, bert.encoder.layer.8.attention.self.dropout, bert.encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS = 312630.66 * L^2 + 2439028736.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a simple regression model for the flops\n",
    "lengths_to_sample = [4, 8, 16, 32, 64, 128, 256]\n",
    "model_1_flops_per_length = {}\n",
    "\n",
    "for L in lengths_to_sample:\n",
    "    tokens = tokenizer(\" \".join([\"hello\"] * L), return_tensors=\"pt\")\n",
    "    flops = FlopCountAnalysis(model, bert_input(tokens)).total()\n",
    "    model_1_flops_per_length[L] = flops\n",
    "\n",
    "# Convert to arrays\n",
    "X = torch.tensor(list(model_1_flops_per_length.keys()), dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(list(model_1_flops_per_length.values()), dtype=torch.float32)\n",
    "\n",
    "# Fit FLOPs = a * L^2 + b\n",
    "model_1_X_squared = X ** 2\n",
    "model_1_X_poly = torch.cat([model_1_X_squared, torch.ones_like(X)], dim=1)\n",
    "model_1_coeffs = torch.linalg.lstsq(model_1_X_poly, y).solution  # returns [a, b]\n",
    "print(f\"FLOPS = {model_1_coeffs[0].item():.2f} * L^2 + {model_1_coeffs[1].item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be1b41",
   "metadata": {},
   "source": [
    "## 3. Evaluating FLOPS for our prefix evaluating MLP (approach 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1e6e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src/\")\n",
    "from confidence_model import confidence_model\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f8d81ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "confidence_model(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model.eval()\n",
    "confidence_model = confidence_model()\n",
    "confidence_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "babcc534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text tokens: torch.Size([1, 768])\n",
      "Small text tokens: torch.Size([1, 768])\n",
      "\n",
      "FLOPS Results:\n",
      "Long text FLOPS CLS: 14,200,709,376\n",
      "Long text FLOPS Model: 196,864\n",
      "Long text FLOPS Total: 14,200,906,240\n",
      "Long text FLOPS/token: 18,490,763.33\n",
      "============================================================\n",
      "Small text FLOPS CLS: 680,835,072\n",
      "Small text FLOPS Model: 196,864\n",
      "Small text FLOPS Total: 681,031,936\n",
      "Small text FLOPS/token: 886,760.33\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate FLOPS for long text given it's input_ids and attention_mask\n",
    "moodel_1_long_flops_prepare = measure_flops(bert_model, bert_input(long_tokens))\n",
    "moodel_1_long_tokens_cls = bert_model(**long_tokens).last_hidden_state[:, 0, :]\n",
    "moodel_1_long_flops_run = measure_flops(confidence_model, moodel_1_long_tokens_cls)\n",
    "moodel_1_long_flops = moodel_1_long_flops_prepare.total() + moodel_1_long_flops_run.total()\n",
    "\n",
    "# Evaluate FLOPS for small text\n",
    "moodel_1_small_flops_prepare = measure_flops(bert_model, bert_input(small_tokens))\n",
    "moodel_1_small_tokens_cls = bert_model(**small_tokens).last_hidden_state[:, 0, :]\n",
    "moodel_1_small_flops_run = measure_flops(confidence_model, moodel_1_small_tokens_cls)\n",
    "moodel_1_small_flops = moodel_1_small_flops_prepare.total() + moodel_1_small_flops_run.total()\n",
    "\n",
    "print(f\"Long text tokens: {moodel_1_long_tokens_cls.shape}\")\n",
    "print(f\"Small text tokens: {moodel_1_small_tokens_cls.shape}\")\n",
    "\n",
    "print(f\"\\nFLOPS Results:\")\n",
    "print(f\"Long text FLOPS CLS: {moodel_1_long_flops_prepare.total():,}\")\n",
    "print(f\"Long text FLOPS Model: {moodel_1_long_flops_run.total():,}\")\n",
    "print(f\"Long text FLOPS Total: {moodel_1_long_flops:,}\")\n",
    "print(f\"Long text FLOPS/token: {moodel_1_long_flops / moodel_1_long_tokens_cls.numel():,.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Small text FLOPS CLS: {moodel_1_small_flops_prepare.total():,}\")\n",
    "print(f\"Small text FLOPS Model: {moodel_1_small_flops_run.total():,}\")\n",
    "print(f\"Small text FLOPS Total: {moodel_1_small_flops:,}\")\n",
    "print(f\"Small text FLOPS/token: {moodel_1_small_flops / moodel_1_small_tokens_cls.numel():,.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c44b1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPS = 312605.25 * L^2 + 2439616512.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a simple regression model for the flops\n",
    "lengths_to_sample = [4, 8, 16, 32, 64, 128, 256]\n",
    "model_1_flops_per_length = {}\n",
    "\n",
    "for L in lengths_to_sample:\n",
    "    tokens = tokenizer(\" \".join([\"hello\"] * L), return_tensors=\"pt\")\n",
    "    cls_token = bert_model(**tokens).last_hidden_state[:, 0, :]\n",
    "    flops_prepare = measure_flops(bert_model, bert_input(tokens)).total()\n",
    "    flops_run = measure_flops(confidence_model, cls_token).total()\n",
    "    flops = flops_prepare + flops_run\n",
    "    model_1_flops_per_length[L] = flops\n",
    "\n",
    "# Convert to arrays\n",
    "X = torch.tensor(list(model_1_flops_per_length.keys()), dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(list(model_1_flops_per_length.values()), dtype=torch.float32)\n",
    "\n",
    "# Fit FLOPs = a * L^2 + b\n",
    "model_1_X_squared = X ** 2\n",
    "model_1_X_poly = torch.cat([model_1_X_squared, torch.ones_like(X)], dim=1)\n",
    "model_1_coeffs = torch.linalg.lstsq(model_1_X_poly, y).solution  # returns [a, b]\n",
    "print(f\"FLOPS = {model_1_coeffs[0].item():.2f} * L^2 + {model_1_coeffs[1].item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0174c",
   "metadata": {},
   "source": [
    "## 4. Evaluating FLOPS for our sliding window evaluating MLP (approach 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11ced94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from window_slide_model import WindowSlideModel\n",
    "window_model = WindowSlideModel(input_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d43d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::embedding encountered 3 time(s)\n",
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::rsub encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::gelu encountered 12 time(s)\n",
      "Unsupported operator aten::tanh encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layer.0.attention.self.dropout, encoder.layer.1.attention.self.dropout, encoder.layer.10.attention.self.dropout, encoder.layer.11.attention.self.dropout, encoder.layer.2.attention.self.dropout, encoder.layer.3.attention.self.dropout, encoder.layer.4.attention.self.dropout, encoder.layer.5.attention.self.dropout, encoder.layer.6.attention.self.dropout, encoder.layer.7.attention.self.dropout, encoder.layer.8.attention.self.dropout, encoder.layer.9.attention.self.dropout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text tokens: torch.Size([1, 768])\n",
      "\n",
      "FLOPS Results:\n",
      "Model 2 FLOPS CLS: 680,835,072\n",
      "Model 2 FLOPS Model: 98,432\n",
      "Model 2 FLOPS Total: 680,933,504\n",
      "Model 2 FLOPS/token: 886,632.17\n"
     ]
    }
   ],
   "source": [
    "model_2_text = \" \".join(long_ner_text.split(\" \")[0:6])\n",
    "model_2_tokens = tokenizer(model_2_text, return_tensors=\"pt\")\n",
    "moodel_2_long_flops_prepare = measure_flops(bert_model, bert_input(model_2_tokens))\n",
    "moodel_2_long_tokens_cls = bert_model(**model_2_tokens).last_hidden_state[:, 0, :]\n",
    "moodel_2_long_flops_run = measure_flops(window_model, moodel_2_long_tokens_cls)\n",
    "moodel_2_long_flops = moodel_2_long_flops_prepare.total() + moodel_2_long_flops_run.total()\n",
    "\n",
    "print(f\"Long text tokens: {moodel_2_long_tokens_cls.shape}\")\n",
    "\n",
    "print(f\"\\nFLOPS Results:\")\n",
    "print(f\"Model 2 FLOPS CLS: {moodel_2_long_flops_prepare.total():,}\")\n",
    "print(f\"Model 2 FLOPS Model: {moodel_2_long_flops_run.total():,}\")\n",
    "print(f\"Model 2 FLOPS Total: {moodel_2_long_flops:,}\")\n",
    "print(f\"Model 2 FLOPS/token: {moodel_2_long_flops / moodel_2_long_tokens_cls.numel():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bda5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
