{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b877e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline,AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78916b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import convert_predictions\n",
    "from src.streaming import process_ontonotes_example, stream_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8e1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontonotes = load_dataset(\"conll2012_ontonotesv5\", \"english_v12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce31fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentences = 500 # it takes 2 whole days to precompute this...\n",
    "sentences = []\n",
    "for doc in ontonotes[\"train\"]:\n",
    "    for sent in doc['sentences']:\n",
    "        if 'words' in sent and len(sent['words']) > 1:\n",
    "            sentences.append(sent['words'])\n",
    "    if len(sentences) >= max_sentences:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ad513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0ca34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from seqeval.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2611575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dslim/bert-base-NER\"\n",
    "f1_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ba88fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pretrained Models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encoder = AutoModel.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=model_name, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc8350",
   "metadata": {},
   "source": [
    "$$ \\mathcal{L}(f) = \\sum_{t \\in \\mathcal{I}f} \\ell{\\mathrm{NER}}(\\hat{y}_t, y_t) + \\lambda, C(\\mathcal{I}_f), $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da313e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If can encapsulate this, then we need to do it.\n",
    "\n",
    "def run_ner_on_tokens(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    return ner_pipeline(text)\n",
    "\n",
    "def get_f1_label(pred_bio, gold_bio_prefix, threshold):\n",
    "    return int(f1_score([gold_bio_prefix], [pred_bio]) >= threshold)\n",
    "\n",
    "def get_cls_embedding(tokens):\n",
    "    text = \" \".join(tokens)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = encoder(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ffa7f",
   "metadata": {},
   "source": [
    "# Main Part for dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f95f3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 0/749 [00:00<?, ?it/s]c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\seqeval\\metrics\\v1.py:159: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "Processing Sentences: 100%|██████████| 749/749 [23:25<00:00,  1.88s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved dataset with 11829 examples from 749 sentences to ner_trigger_dataset.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "sentence_ids = []\n",
    "sentence_lengths = []\n",
    "\n",
    "sentence_idx = 0  # Sentence counter\n",
    "\n",
    "for sentence in tqdm(sentences, desc=\"Processing Sentences\"):\n",
    "    try:\n",
    "        output = run_ner_on_tokens(sentence)\n",
    "        labels = convert_predictions(sentence, output)\n",
    "\n",
    "        prefix_count = 0  # Count how many prefixes this sentence has\n",
    "\n",
    "        for k in range(1, len(sentence) + 1):\n",
    "            prefix = sentence[:k]\n",
    "            prefix_output = run_ner_on_tokens(prefix)\n",
    "            pred_bio = convert_predictions(prefix, prefix_output)\n",
    "\n",
    "            prefix_bio = labels[:k]\n",
    "            label = get_f1_label(pred_bio, prefix_bio, threshold=f1_threshold)\n",
    "\n",
    "            embedding = get_cls_embedding(prefix)\n",
    "\n",
    "            X.append(embedding)\n",
    "            y.append(label)\n",
    "            sentence_ids.append(sentence_idx)\n",
    "            prefix_count += 1\n",
    "\n",
    "        sentence_lengths.append(prefix_count)\n",
    "        sentence_idx += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping sentence due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "sentence_ids = np.array(sentence_ids)\n",
    "sentence_lengths = np.array(sentence_lengths)\n",
    "\n",
    "np.savez(\"ner_trigger_dataset.npz\", X=X, y=y, ids=sentence_ids, lengths=sentence_lengths)\n",
    "print(f\"\\nSaved dataset with {len(X)} examples from {sentence_idx} sentences to ner_trigger_dataset.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e67c8b",
   "metadata": {},
   "source": [
    "# Model Specific Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b13d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing stuff\n",
    "\n",
    "def load_ner_trigger_dataset(path=\"ner_trigger_dataset.npz\"):\n",
    "    data = np.load(path)\n",
    "    X = data[\"X\"]                 # CLS embeddings (num_prefixes, 768)\n",
    "    y = data[\"y\"]                 # Labels (0 or 1)\n",
    "    sentence_ids = data[\"ids\"]    # Sentence ID for each prefix\n",
    "    sentence_lengths = data[\"lengths\"]  # Number of prefixes per sentence\n",
    "    return X, y, sentence_ids, sentence_lengths\n",
    "\n",
    "X, y, sentence_ids, sentence_lengths = load_ner_trigger_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f088f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prefixes: 11829\n",
      "Total sentences: 749\n",
      "Sentence 0 has 5 prefixes.\n",
      "First 5 sentence IDs: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total prefixes: {len(X)}\")\n",
    "print(f\"Total sentences: {len(sentence_lengths)}\")\n",
    "print(f\"Sentence 0 has {sentence_lengths[0]} prefixes.\")\n",
    "print(f\"First 5 sentence IDs: {sentence_ids[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552f0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixDataset(Dataset):\n",
    "    def __init__(self, X, y, sentence_ids):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.ids = torch.tensor(sentence_ids, dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.ids[idx]\n",
    "\n",
    "dataset = PrefixDataset(X, y, sentence_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb726a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERTriggerLoss(nn.Module):\n",
    "    def __init__(self, sentence_lengths):\n",
    "        super().__init__()\n",
    "        self.sentence_lengths = sentence_lengths  \n",
    "\n",
    "    def forward(self, predictions, targets, sentence_ids):\n",
    "        \"\"\"\n",
    "        predictions: tensor of shape (batch_size,) — model outputs (logits or probs)\n",
    "        targets:     tensor of shape (batch_size,) — 0 or 1\n",
    "        sentence_ids: tensor of shape (batch_size,) — maps each prefix to sentence index\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for i in range(len(predictions)):\n",
    "            pred = predictions[i]\n",
    "            target = targets[i]\n",
    "            sent_id = sentence_ids[i].item()\n",
    "\n",
    "            # Get total length of that sentence\n",
    "            total_len = self.sentence_lengths[sent_id]\n",
    "            if total_len == 0:\n",
    "                continue  # skip edge case\n",
    "\n",
    "            # Compute current position within the sentence\n",
    "            prefix_pos = (sentence_ids[:i] == sent_id).sum().item()\n",
    "\n",
    "            # Delay penalty is linear here. maybe consider something else \n",
    "            delay_penalty = prefix_pos / total_len\n",
    "\n",
    "            # Standard BCE loss\n",
    "            bce = F.binary_cross_entropy_with_logits(pred, target.float(), reduction=\"none\")\n",
    "\n",
    "            # If the target is 1 (we should trigger), penalize late triggers\n",
    "            if target == 1:\n",
    "                # Penalize early triggers more — inverse of delay\n",
    "                early_penalty = (1.0 - delay_penalty)\n",
    "\n",
    "                # increase penatly here. atm it is 20%\n",
    "                if delay_penalty <= 0.2:\n",
    "                    early_penalty *= 2.0  # here double, but set as needed\n",
    "\n",
    "                weighted_loss = bce * early_penalty\n",
    "\n",
    "            else:\n",
    "                weighted_loss = bce  \n",
    "\n",
    "            losses.append(weighted_loss)\n",
    "\n",
    "        return torch.stack(losses).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e61595",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = NERTriggerLoss(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd2b4587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 — Loss: 0.7092\n",
      "Epoch 2/20 — Loss: 0.7091\n",
      "Epoch 3/20 — Loss: 0.7097\n",
      "Epoch 4/20 — Loss: 0.7081\n",
      "Epoch 5/20 — Loss: 0.7055\n",
      "Epoch 6/20 — Loss: 0.7066\n",
      "Epoch 7/20 — Loss: 0.7046\n",
      "Epoch 8/20 — Loss: 0.7067\n",
      "Epoch 9/20 — Loss: 0.7048\n",
      "Epoch 10/20 — Loss: 0.7030\n",
      "Epoch 11/20 — Loss: 0.7026\n",
      "Epoch 12/20 — Loss: 0.7020\n",
      "Epoch 13/20 — Loss: 0.7023\n",
      "Epoch 14/20 — Loss: 0.7017\n",
      "Epoch 15/20 — Loss: 0.7042\n",
      "Epoch 16/20 — Loss: 0.7031\n",
      "Epoch 17/20 — Loss: 0.7035\n",
      "Epoch 18/20 — Loss: 0.7022\n",
      "Epoch 19/20 — Loss: 0.7007\n",
      "Epoch 20/20 — Loss: 0.7031\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_batch, id_batch in dataloader:\n",
    "        preds = model(X_batch).squeeze()\n",
    "        loss = criterion(preds, y_batch, id_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} — Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c899ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0:\n",
      "Ground Truth Trigger : None\n",
      "Model Trigger        : None\n",
      "Prediction Sequence  : [0, 0, 0, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 1:\n",
      "Ground Truth Trigger : 5\n",
      "Model Trigger        : 5\n",
      "Prediction Sequence  : [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 2:\n",
      "Ground Truth Trigger : 5\n",
      "Model Trigger        : 0\n",
      "Prediction Sequence  : [1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "Ground Truth Sequence: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 3:\n",
      "Ground Truth Trigger : 2\n",
      "Model Trigger        : 0\n",
      "Prediction Sequence  : [1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "Ground Truth Sequence: [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 4:\n",
      "Ground Truth Trigger : 1\n",
      "Model Trigger        : 1\n",
      "Prediction Sequence  : [0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 5:\n",
      "Ground Truth Trigger : None\n",
      "Model Trigger        : 2\n",
      "Prediction Sequence  : [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 6:\n",
      "Ground Truth Trigger : 0\n",
      "Model Trigger        : 0\n",
      "Prediction Sequence  : [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1]\n",
      "Ground Truth Sequence: [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 7:\n",
      "Ground Truth Trigger : 0\n",
      "Model Trigger        : 0\n",
      "Prediction Sequence  : [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "Ground Truth Sequence: [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 8:\n",
      "Ground Truth Trigger : 0\n",
      "Model Trigger        : 0\n",
      "Prediction Sequence  : [1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1]\n",
      "Ground Truth Sequence: [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 9:\n",
      "Ground Truth Trigger : None\n",
      "Model Trigger        : None\n",
      "Prediction Sequence  : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 10:\n",
      "Ground Truth Trigger : 1\n",
      "Model Trigger        : 1\n",
      "Prediction Sequence  : [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "Ground Truth Sequence: [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 11:\n",
      "Ground Truth Trigger : 2\n",
      "Model Trigger        : 2\n",
      "Prediction Sequence  : [0, 0, 1, 1, 0, 1, 1, 1]\n",
      "Ground Truth Sequence: [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 12:\n",
      "Ground Truth Trigger : 0\n",
      "Model Trigger        : 0\n",
      "Prediction Sequence  : [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Ground Truth Sequence: [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 13:\n",
      "Ground Truth Trigger : 1\n",
      "Model Trigger        : 1\n",
      "Prediction Sequence  : [0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "Ground Truth Sequence: [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 14:\n",
      "Ground Truth Trigger : 1\n",
      "Model Trigger        : 1\n",
      "Prediction Sequence  : [0, 1, 0, 0, 1, 1, 0, 1]\n",
      "Ground Truth Sequence: [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 15:\n",
      "Ground Truth Trigger : 0\n",
      "Model Trigger        : 0\n",
      "Prediction Sequence  : [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "Ground Truth Sequence: [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 16:\n",
      "Ground Truth Trigger : 1\n",
      "Model Trigger        : 1\n",
      "Prediction Sequence  : [0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1]\n",
      "Ground Truth Sequence: [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 17:\n",
      "Ground Truth Trigger : 2\n",
      "Model Trigger        : 2\n",
      "Prediction Sequence  : [0, 0, 1, 1, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 0.0, 1.0, 1.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 18:\n",
      "Ground Truth Trigger : 5\n",
      "Model Trigger        : 5\n",
      "Prediction Sequence  : [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n",
      "Sentence 19:\n",
      "Ground Truth Trigger : 1\n",
      "Model Trigger        : 1\n",
      "Prediction Sequence  : [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Ground Truth Sequence: [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions_by_sentence = {}\n",
    "truths_by_sentence = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch, id_batch in dataloader:\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        preds_binary = (outputs >= 0.5).int()\n",
    "\n",
    "        for i in range(len(X_batch)):\n",
    "            sid = id_batch[i].item()\n",
    "            if sid not in predictions_by_sentence:\n",
    "                predictions_by_sentence[sid] = []\n",
    "                truths_by_sentence[sid] = []\n",
    "            predictions_by_sentence[sid].append(preds_binary[i].item())\n",
    "            truths_by_sentence[sid].append(y_batch[i].item())\n",
    "\n",
    "# Print examples\n",
    "for sid in sorted(predictions_by_sentence.keys())[:20]:\n",
    "    pred_seq = predictions_by_sentence[sid]\n",
    "    truth_seq = truths_by_sentence[sid]\n",
    "    pred_trigger = next((i for i, val in enumerate(pred_seq) if val == 1), None)\n",
    "    true_trigger = next((i for i, val in enumerate(truth_seq) if val == 1), None)\n",
    "\n",
    "    print(f\"Sentence {sid}:\")\n",
    "    print(f\"Ground Truth Trigger : {true_trigger}\")\n",
    "    print(f\"Model Trigger        : {pred_trigger}\")\n",
    "    print(f\"Prediction Sequence  : {pred_seq}\")\n",
    "    print(f\"Ground Truth Sequence: {truth_seq}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3d9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
