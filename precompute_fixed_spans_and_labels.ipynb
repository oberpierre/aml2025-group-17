{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafffec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierre/projects/uzh_repos/aml2025-group-17/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d740b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English portion of OntoNotes 5.0\n",
    "ontonotes = load_dataset(\n",
    "    \"conll2012_ontonotesv5\",\n",
    "    \"english_v12\",\n",
    "    cache_dir=\"./dataset/ontonotes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9f9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_span_labels(words, named_entities, span_length=6):\n",
    "    \"\"\"\n",
    "    Generate spans and binary labels indicating whether each span contains a complete named entity.\n",
    "    \n",
    "    Args:\n",
    "        words: List of words in the sentence\n",
    "        named_entities: List of named entity labels (odd = begin entity, even = continue entity, 0 = no entity)\n",
    "        span_length: Fixed length of spans to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (span_words, label) where label is 0 (no/incomplete entity) or 1 (complete entity)\n",
    "    \"\"\"\n",
    "    spans_and_labels = []\n",
    "    \n",
    "    # Only generate spans of exactly span_length\n",
    "    for i in range(len(words) - span_length + 1):\n",
    "        span_end = i + span_length\n",
    "        span_words = words[i:span_end]\n",
    "        span_entities = named_entities[i:span_end]\n",
    "        \n",
    "        # Check if span contains a complete named entity\n",
    "        has_complete_entity = False\n",
    "        \n",
    "        # Find all entity starts (odd numbers) in the span\n",
    "        entity_starts = [j for j, ne in enumerate(span_entities) if ne % 2 == 1 and ne > 0]\n",
    "        \n",
    "        for start_idx in entity_starts:\n",
    "            # Check if this entity is complete within the span\n",
    "            entity_complete = True\n",
    "            \n",
    "            # Look ahead from the start to see if entity continues\n",
    "            for k in range(start_idx + 1, len(span_entities)):\n",
    "                if span_entities[k] == 0:  # Entity ended\n",
    "                    break\n",
    "                elif span_entities[k] % 2 == 1:  # New entity started\n",
    "                    break\n",
    "                # If we reach here, it's an even number (continuation)\n",
    "            \n",
    "            # Check if entity continues beyond the span\n",
    "            if span_end < len(named_entities):\n",
    "                next_entity = named_entities[span_end]\n",
    "                if next_entity % 2 == 0 and next_entity > 0:  # Entity continues beyond span\n",
    "                    entity_complete = False\n",
    "            \n",
    "            if entity_complete:\n",
    "                has_complete_entity = True\n",
    "                break\n",
    "        \n",
    "        label = 1 if has_complete_entity else 0\n",
    "        spans_and_labels.append((span_words, label))\n",
    "    \n",
    "    return spans_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 10539/10539 [00:09<00:00, 1170.13it/s] \n",
      "Processing Documents: 100%|██████████| 10539/10539 [00:09<00:00, 1170.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved dataset with 2148223 examples to ner_trigger_dataset.npz\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "SPAN_LENGTH = 6\n",
    "\n",
    "for doc in tqdm(ontonotes['train'], desc=\"Processing Documents\"):\n",
    "    # Concatenate all words and named entities from all sentences in the document\n",
    "    words = []\n",
    "    named_entities = []\n",
    "    \n",
    "    for sentence in doc['sentences']:\n",
    "        words.extend(sentence['words'])\n",
    "        named_entities.extend(sentence['named_entities'])\n",
    "    \n",
    "    # Generate spans and labels for the entire document\n",
    "    spans_and_labels = generate_span_labels(words, named_entities, SPAN_LENGTH)\n",
    "    \n",
    "    for span_words, label in spans_and_labels:\n",
    "        X.append(span_words)\n",
    "        y.append(label)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "np.savez(\"ner_trigger_dataset.npz\", X=X, y=y)\n",
    "print(f\"\\nSaved dataset with {len(X)} examples to ner_trigger_dataset.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd1b2360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_trigger_dataset.npz is too large for git, therefore it's available online at https://drive.google.com/drive/folders/1ykTaDLdHIEmZQYN0b1Hr9hkOYjgMshSa?usp=sharing\n"
     ]
    }
   ],
   "source": [
    "print(\"ner_trigger_dataset.npz is too large for git, therefore it's available online at https://drive.google.com/drive/folders/1ykTaDLdHIEmZQYN0b1Hr9hkOYjgMshSa?usp=sharing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e62ae",
   "metadata": {},
   "source": [
    "## Precomputing CLS token for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdc7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0691fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2148223, 6) (2148223,)\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/ner_trigger_dataset.npz')\n",
    "X, y = data['X'], data['y']\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0cca28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tolist()  # Convert to list for tokenizer compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ccc43f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|██████████| 8392/8392 [1:02:42<00:00,  2.23it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved dataset with 2148223 examples to ner_trigger_dataset_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Calculate token embeddings for each span in a batched way\n",
    "def get_span_embeddings(X, model, tokenizer, device, BATCH_SIZE=256):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(X), BATCH_SIZE), desc=\"Calculating embeddings\"):\n",
    "            batch = X[i:i + BATCH_SIZE]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=True).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            cls = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(cls)\n",
    "    return np.concatenate(all_embeddings)\n",
    "\n",
    "cls_tokens = get_span_embeddings(X, model, tokenizer, device)\n",
    "\n",
    "np.savez(\"data/ner_trigger_dataset_embeddings.npz\", X=cls_tokens, y=y)\n",
    "print(f\"\\nSaved dataset with {len(X)} examples to ner_trigger_dataset_embeddings.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04f7d0",
   "metadata": {},
   "source": [
    "## Evaluating all windows in ontonotes test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01fd8fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 1200/1200 [00:00<00:00, 1360.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of windows in test dataset: 224128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_windows = 0\n",
    "SPAN_LENGTH = 6\n",
    "\n",
    "for doc in tqdm(ontonotes['test'], desc=\"Processing Documents\"):\n",
    "    # Concatenate all words and named entities from all sentences in the document\n",
    "    words = []\n",
    "    named_entities = []\n",
    "    \n",
    "    for sentence in doc['sentences']:\n",
    "        words.extend(sentence['words'])\n",
    "        named_entities.extend(sentence['named_entities'])\n",
    "    \n",
    "    # Generate spans and labels for the entire document\n",
    "    spans_and_labels = generate_span_labels(words, named_entities, SPAN_LENGTH)\n",
    "    num_windows += len(spans_and_labels)\n",
    "\n",
    "print(f\"Total number of windows in test dataset: {num_windows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef815a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
